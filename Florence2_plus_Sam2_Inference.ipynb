{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Florence2 + SAM2 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.ops import masks_to_boxes\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import supervision as sv\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    get_scheduler\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from tqdm import tqdm\n",
    "from torchvision.ops import masks_to_boxes\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "import random\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Global Variables\n",
    "## - Florence2\n",
    "data_dir= Path(\"./snemi/\" )\n",
    "raw_image_dir = data_dir / 'image_pngs'\n",
    "seg_image_dir = data_dir / 'seg_pngs'\n",
    "test_image_dir = data_dir / 'image_test_pngs'\n",
    "max_output_token = 2048\n",
    "\n",
    "\n",
    "CHECKPOINT = \"microsoft/Florence-2-base-ft\"\n",
    "REVISION = 'refs/pr/6'\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "label_num = 170\n",
    "\n",
    "## - Define Dataloader\n",
    "BATCH_SIZE = 5\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "checkpoint_dir = './model_checkpoints/large_model/epoch_700'\n",
    "\n",
    "## - SAM2\n",
    "\n",
    "data_dir= Path(\"./snemi/\" )\n",
    "raw_image_dir = data_dir / 'image_pngs'\n",
    "seg_image_dir = data_dir / 'seg_pngs'\n",
    "test_image_dir = data_dir / 'image_test_pngs'\n",
    "test_image_slice_dir = data_dir / \"image_slice_test_pngs\"\n",
    "test_image_pred_dir = data_dir / \"image_pred_test_pngs\"\n",
    "\n",
    "os.makedirs(test_image_slice_dir, exist_ok=True)\n",
    "os.makedirs(test_image_pred_dir, exist_ok=True)\n",
    "sam2_checkpoint = \"./sam2_hiera_large.pt\"\n",
    "model_cfg = \"./sam2_hiera_l.yaml\"\n",
    "overlapp_ratio = 0.25\n",
    "finetuned_parameter_path = \"./checkpoints/all/large_model_full_1000.torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Load Models\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_dir, trust_remote_code=True).to(DEVICE)\n",
    "processor = AutoProcessor.from_pretrained(checkpoint_dir, trust_remote_code=True)\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=DEVICE)\n",
    "predictor = SAM2ImagePredictor(sam2_model)\n",
    "predictor.model.load_state_dict(torch.load(finetuned_parameter_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - slice test images\n",
    "all_files = np.sort(os.listdir(test_image_dir))\n",
    "test_image_path_lst = np.array([test_image_dir / test_image_path for test_image_path in all_files])\n",
    "\n",
    "# - slice image and segmentation for florence sequence length limit\n",
    "def create_slices(image_path, slice_image_dir):\n",
    "    img = Image.open(image_path)\n",
    "     # Get image dimensions\n",
    "    width, height = img.size\n",
    "    \n",
    "    # Calculate the midpoint\n",
    "    mid_x, mid_y = width // 2, height // 2\n",
    "    \n",
    "    # Define the four slices (left, upper, right, lower)\n",
    "    slices = {\n",
    "        'top_left': (0, 0, mid_x, mid_y),\n",
    "        'top_right': (mid_x, 0, width, mid_y),\n",
    "        'bottom_left': (0, mid_y, mid_x, height),\n",
    "        'bottom_right': (mid_x, mid_y, width, height)\n",
    "    }\n",
    "    \n",
    "    # Loop through the slices, crop, and save them\n",
    "    all_slices = []\n",
    "    for key, coords in slices.items():\n",
    "        slice_img = img.crop(coords)\n",
    "        # Format the name: base name + coordinates\n",
    "        slice_filename = f\"{image_path.stem}_{coords[0]}_{coords[1]}_{coords[2]}_{coords[3]}.png\"\n",
    "        slice_img.save( slice_image_dir / slice_filename)\n",
    "        all_slices.append( slice_image_dir / slice_filename)\n",
    "\n",
    "    return all_slices\n",
    "\n",
    "all_test_slices = []\n",
    "\n",
    "for slice_path in test_image_path_lst:\n",
    "    all_test_slices += create_slices(slice_path, test_image_slice_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - check invalid bounding boxes\n",
    "def validate_boxes(boxes):\n",
    "    \"\"\"Validate and filter out invalid bounding boxes.\"\"\"\n",
    "    valid_boxes = []\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = box[:4]\n",
    "        if x_max > x_min and y_max > y_min:\n",
    "            valid_boxes.append(box)\n",
    "    return valid_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Florence2 and Sam2 Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Florence2 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Define Florence Inference\n",
    "def flo_infer_bboxes(image_path, max_new_tokens, num_steps = 1):\n",
    "    max_labels = 0\n",
    "    max_bboxes = []\n",
    "    max_image = []\n",
    "    for i in tqdm(range(num_steps), desc=\"Florence2 Prediction\", leave=False):\n",
    "        image = cv2.imread(str(image_path))\n",
    "        image = Image.fromarray(image)\n",
    "        task = \"<OD>\"\n",
    "        text = \"<OD>\"\n",
    "        inputs = processor(\n",
    "            text=text, \n",
    "            images=image, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            pixel_values=inputs[\"pixel_values\"],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=3)\n",
    "        generated_text = processor.batch_decode(\n",
    "            generated_ids, skip_special_tokens=False)[0]\n",
    "        response = processor.post_process_generation(\n",
    "            generated_text, \n",
    "            task=task, \n",
    "            image_size=image.size)\n",
    "        detections = sv.Detections.from_lmm(\n",
    "            sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n",
    "\n",
    "        bounding_box_annotator = sv.BoxAnnotator(\n",
    "            color_lookup=sv.ColorLookup.INDEX)\n",
    "        label_annotator = sv.LabelAnnotator(\n",
    "            color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "        image = bounding_box_annotator.annotate(image, detections)\n",
    "        image = label_annotator.annotate(image, detections)\n",
    "        if len(response['<OD>']['bboxes']) > max_labels:\n",
    "            max_labels = len(response['<OD>']['bboxes'])\n",
    "            max_bboxes = response['<OD>']['bboxes']\n",
    "            max_image = image\n",
    "    plt.imshow(max_image)\n",
    "    return max_labels, max_bboxes, max_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAM2 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - show box and show seg \n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25) \n",
    "\n",
    "def show_seg(img, seg_map, coords = None, labels = None):\n",
    "     # - project back to RGB\n",
    "    rgb_image = np.zeros((seg_map.shape[0], seg_map.shape[1], 3), dtype=np.uint8)\n",
    "    for id_class in range(1,seg_map.max()+1):\n",
    "        rgb_image[seg_map == id_class] = [np.random.randint(255), np.random.randint(255), np.random.randint(255)]\n",
    "\n",
    "    # plt.figure(figsize=(20, 20))\n",
    "\n",
    "    # # First image (annotation)\n",
    "    # if coords is not None and labels is not None:\n",
    "    #     plt.subplot(1, 3, 1)  # 1 row, 3 columns, first subplot\n",
    "    #     plt.imshow(rgb_image)\n",
    "    #     show_points(coords, labels, plt.gca())\n",
    "    #     plt.title('Annotation')\n",
    "    #     plt.axis('off')  # Hide the axis\n",
    "    # else:\n",
    "    #     plt.subplot(1, 3, 1)  # 1 row, 3 columns, first subplot\n",
    "    #     plt.imshow(rgb_image)\n",
    "    #     plt.title('Annotation')\n",
    "    #     plt.axis('off')  # Hide the axis\n",
    "\n",
    "    # # Second image (mix)\n",
    "    # plt.subplot(1, 3, 2)  # 1 row, 3 columns, second subplot\n",
    "    # mix_image = (rgb_image / 2 + img / 2).astype(np.uint8)\n",
    "    # plt.imshow(mix_image)\n",
    "    # plt.title('Mix')\n",
    "    # plt.axis('off')\n",
    "\n",
    "    # # Third image (original image)\n",
    "    # plt.subplot(1, 3, 3)  # 1 row, 3 columns, third subplot\n",
    "    # plt.imshow(img)\n",
    "    # plt.title('Image')\n",
    "    # plt.axis('off')\n",
    "\n",
    "    # plt.tight_layout()  # Adjust the layout so everything fits without overlap\n",
    "    # plt.show()\n",
    "\n",
    "    return rgb_image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - SAM2 Predict Segmentation\n",
    "def predict_seg(\n",
    "    img: np.ndarray,\n",
    "    input_points: np.ndarray = None,\n",
    "    point_labels: np.ndarray = None,\n",
    "    input_boxes: np.ndarray = None,\n",
    "    multimask_output: bool = False\n",
    ") -> np.ndarray:\n",
    "    torch.autocast(device_type='cpu', dtype=torch.bfloat16).__enter__()\n",
    "    with torch.no_grad(): # prevent the net from caclulate gradient (more efficient inference)\n",
    "            predictor.set_image(img) # image encoder\n",
    "            masks, scores, logits = predictor.predict(  # prompt encoder + mask decoder\n",
    "                point_coords=input_points,\n",
    "                point_labels=point_labels,\n",
    "                box = input_boxes,\n",
    "                multimask_output=multimask_output)\n",
    "            \n",
    "    # - sort masks based on their scores (high-quality segmentation)\n",
    "    shorted_masks = masks[np.argsort(scores[:,0])[::-1], :, :, :].astype(bool)\n",
    "    seg_map = np.zeros_like(shorted_masks[0, 0, ...], dtype=np.uint8)\n",
    "    occupancy_mask = np.zeros_like(shorted_masks[0, 0, ...],dtype=bool)\n",
    "\n",
    "    # - add the masks one by one from high to low score\n",
    "    for i in range(shorted_masks.shape[0]):\n",
    "        if multimask_output:\n",
    "            mask_lst = shorted_masks[i]\n",
    "            score_lst = scores[i]\n",
    "            score_rank = np.argsort(score_lst)[::-1]\n",
    "            sorted_mask_lst = mask_lst[score_rank]\n",
    "            for mask in sorted_mask_lst:\n",
    "                if (mask*occupancy_mask).sum()/mask.sum() <= overlapp_ratio:\n",
    "                    mask[occupancy_mask]=0\n",
    "                    seg_map[mask]=i+1\n",
    "                    occupancy_mask[mask]=1\n",
    "                    break\n",
    "        else:\n",
    "             mask = shorted_masks[i, 0, ...]\n",
    "             if (mask*occupancy_mask).sum()/mask.sum() <= overlapp_ratio:\n",
    "                mask[occupancy_mask]=0\n",
    "                seg_map[mask]=i+1\n",
    "                occupancy_mask[mask]=1\n",
    "        \n",
    "    # - project back to RGB\n",
    "    rgb_image = show_seg(img, seg_map)\n",
    "    return occupancy_mask, seg_map, rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Post Predict Segmentation\n",
    "def post_predict_seg(\n",
    "    img: np.ndarray,\n",
    "    occupancy_mask,\n",
    "    seg_map,\n",
    "    input_points: np.ndarray = None,\n",
    "    point_labels: np.ndarray = None,\n",
    "    input_boxes: np.ndarray = None,\n",
    "    multimask_output: bool = False,\n",
    ") -> np.ndarray:\n",
    "    torch.autocast(device_type='cpu', dtype=torch.bfloat16).__enter__()\n",
    "    with torch.no_grad(): # prevent the net from caclulate gradient (more efficient inference)\n",
    "            predictor.set_image(img) # image encoder\n",
    "            masks, scores, logits = predictor.predict(  # prompt encoder + mask decoder\n",
    "                point_coords=input_points,\n",
    "                point_labels=point_labels,\n",
    "                box = input_boxes,\n",
    "                multimask_output=multimask_output)\n",
    "            \n",
    "    # - sort masks based on their scores (high-quality segmentation)\n",
    "    if len(masks.shape) == 3:\n",
    "        masks = masks.reshape((1, masks.shape[0], masks.shape[1], masks.shape[2]))\n",
    "        scores = scores.reshape((1, len(scores)))\n",
    "    shorted_masks = masks[np.argsort(scores[:,0])[::-1], :, :, :].astype(bool)\n",
    "    seg_map = seg_map\n",
    "    occupancy_mask = occupancy_mask\n",
    "    max_id = np.max(seg_map) + 1\n",
    "\n",
    "    # - add the masks one by one from high to low score\n",
    "    for i in range(shorted_masks.shape[0]):\n",
    "        if multimask_output:\n",
    "            mask_lst = shorted_masks[i]\n",
    "            score_lst = scores[i]\n",
    "            score_rank = np.argsort(score_lst)[::-1]\n",
    "            sorted_mask_lst = mask_lst[score_rank]\n",
    "            for mask in sorted_mask_lst:\n",
    "                if (mask*occupancy_mask).sum()/mask.sum() <= overlapp_ratio:\n",
    "                    mask[occupancy_mask]=0\n",
    "                    seg_map[mask]=max_id + i\n",
    "                    occupancy_mask[mask]=1\n",
    "                    break\n",
    "        else:\n",
    "             mask = shorted_masks[i, 0, ...]\n",
    "             if (mask*occupancy_mask).sum()/mask.sum() <= overlapp_ratio:\n",
    "                mask[occupancy_mask]=0\n",
    "                seg_map[mask]=max_id + i\n",
    "                occupancy_mask[mask]=1\n",
    "        \n",
    "    # - project back to RGB\n",
    "    rgb_image = show_seg(img, seg_map)\n",
    "    return occupancy_mask, seg_map, rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Florence2 + SAM2 Test Sample Image: only bboxes\n",
    "image_path = all_test_slices[0]\n",
    "num_labels, bboxes, image = flo_infer_bboxes(image_path, 2048, num_steps = 1)\n",
    "print(f'the number of labels: {num_labels}')\n",
    "\n",
    "# - Fliter Invalid Bound Boxes\n",
    "bboxes = validate_boxes(bboxes)\n",
    "image = cv2.imread(str(image_path))\n",
    "occupancy_mask, seg_map, rgb_image = predict_seg( image , None, None, input_boxes = bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Mask Selection and Duplicate Inference Correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Select Sparse Coordinates\n",
    "\n",
    "def select_sparse_coordinates(occupancy_mask, \n",
    "    erode_kernel_size: tuple = (2, 2),\n",
    "    erode_iterations: int = 10,\n",
    "    dilate_kernel_size: tuple = (2, 2),\n",
    "    dilate_iterations: int = 2,\n",
    "    contour_area_threshold: int = 100\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Processes the input mask by performing erosion, dilation, contour filtering, \n",
    "    and marking centroids of the contours.\n",
    "    \n",
    "    Parameters:\n",
    "    mask (np.ndarray): Input binary mask image.\n",
    "    erode_kernel_size (tuple): Kernel size for erosion.\n",
    "    erode_iterations (int): Number of iterations for erosion.\n",
    "    dilate_kernel_size (tuple): Kernel size for dilation.\n",
    "    dilate_iterations (int): Number of iterations for dilation.\n",
    "    contour_area_threshold (int): Minimum area for contours to be considered valid.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: Processed image with centroids marked.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 3: Erode the mask\n",
    "    img_inv = np.uint8(~occupancy_mask * 255)\n",
    "    kernel = np.ones(erode_kernel_size, np.uint8)\n",
    "    morphed_inv_mask = cv2.erode(img_inv, kernel, iterations=erode_iterations)\n",
    "\n",
    "    # Step 4: Filter contours based on area threshold\n",
    "    filt_inv_mask = np.zeros_like(morphed_inv_mask, dtype=np.uint8)\n",
    "    contours, _ = cv2.findContours(morphed_inv_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    for cntr in contours:\n",
    "        if cv2.contourArea(cntr) > contour_area_threshold:\n",
    "            cv2.drawContours(filt_inv_mask, [cntr], -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "    # Step 5: Dilate the filtered mask\n",
    "    kernel = np.ones(dilate_kernel_size, np.uint8)\n",
    "    morphed_filt_mask = cv2.dilate(filt_inv_mask, kernel, iterations=dilate_iterations)\n",
    "\n",
    "    # Step 6: Find centroids of the remaining contours and collect coordinates\n",
    "    centroids = []\n",
    "    contours, _ = cv2.findContours(morphed_filt_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    for cntr in contours:\n",
    "        M = cv2.moments(cntr)\n",
    "        if M['m00'] != 0:  # Avoid division by zero\n",
    "            cx = int(M['m10'] / M['m00'])\n",
    "            cy = int(M['m01'] / M['m00'])\n",
    "            centroids.append((cx, cy))\n",
    "\n",
    "    # Convert list of centroids to a NumPy array\n",
    "    return np.array(centroids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - flo + sam inference\n",
    "def flo_sam_infer(image_path, num_corr = 5, num_steps = 10):\n",
    "    num_labels, bboxes, image = flo_infer_bboxes(image_path, 2048, num_steps=num_steps)\n",
    "    bboxes = validate_boxes(bboxes)\n",
    "    img = cv2.imread(str(image_path))\n",
    "    occupancy_mask, seg_map, rgb_image = predict_seg( img , None, None, input_boxes = bboxes)\n",
    "\n",
    "    for i in tqdm(range(num_corr), desc='Continuous Correction', leave = False):\n",
    "        false_coordinates = np.array(select_sparse_coordinates(occupancy_mask))\n",
    "        false_labels = np.ones([false_coordinates.shape[0],])\n",
    "\n",
    "        # reshape coordinates and prepare labels\n",
    "        show_seg(img, seg_map, false_coordinates, false_labels)\n",
    "        false_coordinates = false_coordinates.reshape((false_coordinates.shape[0], 1, false_coordinates.shape[1]))\n",
    "        false_labels = np.ones([false_coordinates.shape[0], 1])\n",
    "\n",
    "        occupancy_mask, seg_map, rgb_image = post_predict_seg( img, occupancy_mask, seg_map, input_points = false_coordinates, point_labels = false_labels, multimask_output=True)\n",
    "    \n",
    "    return rgb_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_path = all_test_slices[0]\n",
    "rgb_image = flo_sam_infer(image_path, num_corr = 1, num_steps = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Instance Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - convert to binary mask\n",
    "def convert_binary_mask(masks, image_seg, coords, total_shape):\n",
    "    image_seg = image_seg[...,0]\n",
    "     # get bounding boxes from the mask\n",
    "    inds = np.unique(image_seg)[1:]  # load all indices\n",
    "    xmin, ymin, xmax, ymax = coords\n",
    "\n",
    "    # Prepare an empty mask with the total shape (e.g., 1024x1024)\n",
    "    # Get binary masks and points\n",
    "    \n",
    "    masks = masks\n",
    "    for ind in inds:\n",
    "        total_mask = np.zeros(total_shape, dtype=np.uint8)\n",
    "        mask = (image_seg == ind).astype(np.uint8)\n",
    "        total_mask[ymin:ymax, xmin:xmax] = mask\n",
    "        masks.append(total_mask)\n",
    "    masks = np.array(masks)\n",
    "\n",
    "    mask_tensor = torch.from_numpy(masks)\n",
    "\n",
    "    boxes = masks_to_boxes(mask_tensor)\n",
    "\n",
    "    return masks, boxes.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_sample = \"image0000\"\n",
    "# image_sample_path_lst = all_test_slices[:4]\n",
    "# total_shape = (1024, 1024)\n",
    "# binary_masks = []\n",
    "# for image_path in image_sample_path_lst:\n",
    "#     rgb_seg = flo_sam_infer(image_path, num_corr=10)\n",
    "#     coords = image_path.stem.split(\"_\")[1:]\n",
    "#     coords = [int(x) for x in coords]\n",
    "#     binary_masks = list(binary_masks)\n",
    "#     binary_masks, mask_boxes = convert_binary_mask(binary_masks, rgb_seg, coords, (1024, 1024))\n",
    "\n",
    "\n",
    "# combined_mask = np.sum(binary_masks, axis=0) > 0  # Sum all masks and threshold (1 if any overlap)\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# plt.imshow(combined_mask, cmap='gray')\n",
    "# plt.title('Combined Binary Mask')\n",
    "# plt.axis('off')  # Hide axis labels and ticks\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice Inference Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSlicer:\n",
    "    def __init__(self, image_path: str, slice_size: tuple, overlap_ratio: float, output_dir: str):\n",
    "        self.image_path = image_path\n",
    "        self.slice_size = slice_size\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load the image\n",
    "        self.image = Image.open(self.image_path)\n",
    "        self.image_width, self.image_height = self.image.size\n",
    "\n",
    "    # - slicer padding problem\n",
    "    def slice_image(self):\n",
    "        # Calculate the step size based on the overlap ratio\n",
    "        step_size_x = int(self.slice_size[0] * (1 - self.overlap_ratio))\n",
    "        step_size_y = int(self.slice_size[1] * (1 - self.overlap_ratio))\n",
    "\n",
    "        slice_paths = []\n",
    "        slice_number = 0\n",
    "\n",
    "        # Loop through the image dimensions to create overlapping slices\n",
    "        for y in range(0, self.image_height , step_size_y):\n",
    "            for x in range(0, self.image_width , step_size_x):\n",
    "                # Define the box for cropping\n",
    "                box = [x, y, min(x + self.slice_size[0], self.image_width), \n",
    "                             min(y + self.slice_size[1], self.image_height)]\n",
    "                if box[2] == self.image_width:\n",
    "                    box[0] = self.image_width- self.slice_size[0]\n",
    "                if box[3] == self.image_height:\n",
    "                    box[1] = self.image_height- self.slice_size[1]\n",
    "                slice_img = self.image.crop(box)\n",
    "\n",
    "                # Save the slice with the coordinates in the filename\n",
    "                slice_filename = f\"{os.path.basename(self.image_path).split('.')[0]}_{box[0]}_{box[1]}_{box[2]}_{box[3]}.png\"\n",
    "                slice_path = os.path.join(self.output_dir, slice_filename)\n",
    "                \n",
    "                # Save the slice and append to the list of paths\n",
    "                slice_img.save(slice_path)\n",
    "                slice_paths.append(Path(slice_path))\n",
    "\n",
    "                slice_number += 1\n",
    "\n",
    "        return slice_paths\n",
    "\n",
    "    def display_images_in_grid(self, image_paths):\n",
    "        num_images = len(image_paths)\n",
    "    \n",
    "        # Determine the grid size: (cols, rows)\n",
    "        cols = int(num_images**0.5)\n",
    "        rows = (num_images // cols) + (num_images % cols > 0)\n",
    "    \n",
    "        # Create a figure with subplots\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "    \n",
    "        # Flatten axes array in case of multiple rows and columns\n",
    "        axes = axes.flatten() if num_images > 1 else [axes]\n",
    "    \n",
    "        for i, image_path in enumerate(image_paths):\n",
    "            img = Image.open(image_path)  # Open the image\n",
    "            axes[i].imshow(img)           # Display the image\n",
    "            axes[i].axis('off')           # Hide axes for better appearance\n",
    "    \n",
    "        # Remove any unused subplots (if the grid is larger than the number of images)\n",
    "        for ax in axes[num_images:]:\n",
    "            ax.remove()\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def convert_binary_mask(masks, image_seg, coords, total_shape):\n",
    "        image_seg = image_seg[...,0]\n",
    "        # get bounding boxes from the mask\n",
    "        inds = np.unique(image_seg)[1:]  # load all indices\n",
    "        xmin, ymin, xmax, ymax = coords\n",
    "\n",
    "        # Prepare an empty mask with the total shape (e.g., 1024x1024)\n",
    "        # Get binary masks and points\n",
    "    \n",
    "        masks = masks\n",
    "        for ind in inds:\n",
    "            total_mask = np.zeros(total_shape, dtype=np.uint8)\n",
    "            mask = (image_seg == ind).astype(np.uint8)\n",
    "            total_mask[ymin:ymax, xmin:xmax] = mask\n",
    "            masks.append(total_mask)\n",
    "        masks = np.array(masks)\n",
    "\n",
    "        mask_tensor = torch.from_numpy(masks)\n",
    "\n",
    "        boxes = masks_to_boxes(mask_tensor)\n",
    "\n",
    "        return masks, boxes.numpy()\n",
    "\n",
    "    def run_segmentation(self, slice_paths, total_shape, num_corr = 10, num_steps = 1):\n",
    "        binary_masks = []\n",
    "        for image_path in tqdm(slice_paths, desc = \"Running Segmentation\"):\n",
    "            rgb_seg = flo_sam_infer(image_path, num_corr = num_corr, num_steps = num_steps)\n",
    "            coords = image_path.stem.split(\"_\")[1:]\n",
    "            coords = [int(x) for x in coords]\n",
    "            binary_masks = list(binary_masks)\n",
    "            binary_masks, mask_boxes = convert_binary_mask(binary_masks, rgb_seg, coords, total_shape)\n",
    "        combined_mask = np.sum(binary_masks, axis=0) > 0  # Sum all masks and threshold (1 if any overlap)\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        plt.imshow(combined_mask, cmap='gray')\n",
    "        plt.title('Combined Binary Mask')\n",
    "        plt.axis('off')  # Hide axis labels and ticks\n",
    "        plt.show()\n",
    "        \n",
    "        return combined_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sample_slicer = ImageSlicer(test_image_path_lst[0], (512, 512), 0.5, test_image_slice_dir)\n",
    "slice_lst = image_sample_slicer.slice_image()\n",
    "image_sample_slicer.display_images_in_grid(slice_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_binary_mask = image_sample_slicer.run_segmentation(slice_lst, (1024, 1024), num_corr=10, num_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_binary_image = Image.fromarray(full_binary_mask)\n",
    "# img_id = re.findall(r'\\d+', image_sample_slicer.image_path.stem)\n",
    "\n",
    "# temp_path = test_image_pred_dir / ('seg' + img_id[0] + '.png')\n",
    "# full_binary_image.save(temp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Image Prediction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erode_dilate_to_connect(mask: np.ndarray, kernel_size: int = 10, max_iters: int = 20) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Iteratively applies erosion and dilation until the mask is a single connected component.\n",
    "    \n",
    "    Args:\n",
    "    - mask (np.ndarray): A binary mask where 1 represents the object and 0 represents the background.\n",
    "    - kernel_size (int): Size of the structuring element for erosion and dilation.\n",
    "    - max_iters (int): Maximum number of iterations to apply erosion and dilation.\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray: The mask with connected components merged.\n",
    "    \"\"\"\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        # Erode the mask\n",
    "        eroded_mask = cv2.erode(mask, kernel, iterations=1)\n",
    "        \n",
    "        # Check if the mask is now connected\n",
    "        num_labels, _ = cv2.connectedComponents(eroded_mask)\n",
    "        \n",
    "        if num_labels == 2:  # One background and one connected object\n",
    "            return cv2.dilate(eroded_mask, kernel, iterations=1)  # Restore shape with dilation\n",
    "        \n",
    "        # If not connected, continue erosion and dilation\n",
    "        mask = cv2.dilate(eroded_mask, kernel, iterations=1)\n",
    "    \n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_small_points(mask, min_area = 50):\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask.astype(np.uint8))\n",
    "    filtered_mask = np.zeros_like(mask, dtype=np.uint8)\n",
    "    for label in range(1, num_labels):  # Start from 1 to ignore the background\n",
    "        area = stats[label, cv2.CC_STAT_AREA]\n",
    "        if area >= min_area:  # Keep only components with area larger than the threshold\n",
    "            filtered_mask[labels == label] = 1\n",
    "    return filtered_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_fully_connected(mask, kernel_size = 30, max_iterations=1):\n",
    "    for i in range(max_iterations):\n",
    "        kernel = np.ones((kernel_size, kernel_size))\n",
    "        kernel_size += 1\n",
    "        # Apply morphological closing\n",
    "        new_mask = cv2.morphologyEx(mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n",
    "        # num_labels, _ = cv2.connectedComponents(new_mask)\n",
    "        # if num_labels == 2:\n",
    "        #     return new_mask\n",
    "\n",
    "    \n",
    "    # # Create a subplot with two images: the original and the new mask\n",
    "    # plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # # Plot the original mask\n",
    "    # plt.subplot(1, 2, 1)  # 1 row, 2 columns, first plot\n",
    "    # plt.imshow(mask, cmap='gray')\n",
    "    # plt.title(\"Original Mask\")\n",
    "    # plt.axis('off')  # Hide axis ticks\n",
    "    \n",
    "    # # Plot the new mask\n",
    "    # plt.subplot(1, 2, 2)  # 1 row, 2 columns, second plot\n",
    "    # plt.imshow(new_mask, cmap='gray')\n",
    "    # plt.title(\"New Mask After Morphology\")\n",
    "    # plt.axis('off')  # Hide axis ticks\n",
    "    \n",
    "    # # Show the plot\n",
    "    # plt.show()\n",
    "\n",
    "    # print(f\"Mask is not connected after iters {max_iterations}\")\n",
    "    \n",
    "    return new_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_predict_seg(\n",
    "    img: np.ndarray,\n",
    "    occupancy_mask = None,\n",
    "    seg_map = None,\n",
    "    input_points: np.ndarray = None,\n",
    "    point_labels: np.ndarray = None,\n",
    "    input_boxes: np.ndarray = None,\n",
    "    multimask_output: bool = False,\n",
    "    filter = True,\n",
    "    min_area: int = 50,\n",
    "    overlap_ratio = 0.2\n",
    ") -> np.ndarray:\n",
    "    torch.autocast(device_type='cpu', dtype=torch.bfloat16).__enter__()\n",
    "    with torch.no_grad(): # prevent the net from caclulate gradient (more efficient inference)\n",
    "            predictor.set_image(img) # image encoder\n",
    "            masks, scores, logits = predictor.predict(  # prompt encoder + mask decoder\n",
    "                point_coords=input_points,\n",
    "                point_labels=point_labels,\n",
    "                box = input_boxes,\n",
    "                multimask_output=multimask_output)\n",
    "            \n",
    "    # - sort masks based on their scores (high-quality segmentation)\n",
    "    shorted_masks = masks[np.argsort(scores[:,0])[::-1], :, :, :].astype(bool)\n",
    "    if seg_map is None and occupancy_mask is None:\n",
    "        seg_map = np.zeros_like(shorted_masks[0, 0, ...], dtype=np.uint16)\n",
    "        occupancy_mask = np.zeros_like(shorted_masks[0, 0, ...],dtype=bool)\n",
    "    else:\n",
    "        seg_map = seg_map\n",
    "        occupancy_mask = occupancy_mask\n",
    "    max_id = np.max(seg_map) + 1\n",
    "\n",
    "    # - add the masks one by one from high to low score\n",
    "    id_increase = 0\n",
    "    for i in range(shorted_masks.shape[0]):\n",
    "        if multimask_output:\n",
    "            mask_lst = shorted_masks[i]\n",
    "            score_lst = scores[i]\n",
    "            score_rank = np.argsort(score_lst)[::-1]\n",
    "            sorted_mask_lst = mask_lst[score_rank]\n",
    "            for mask in sorted_mask_lst:\n",
    "                if (mask*occupancy_mask).sum()/mask.sum() <= overlapp_ratio:\n",
    "                    if filter:\n",
    "                        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask.astype(np.uint8))\n",
    "                        if num_labels != 2:\n",
    "                            continue\n",
    "                        else:\n",
    "                            mask = is_fully_connected(mask, kernel_size=30)\n",
    "                            mask = mask.astype(bool)\n",
    "                    else:\n",
    "                        # Apply morphological closing (dilation followed by erosion)\n",
    "                        mask = is_fully_connected(mask, kernel_size=150)\n",
    "                        mask = mask.astype(bool)\n",
    "                            \n",
    "                    mask[occupancy_mask]=0\n",
    "                    seg_map[mask]=max_id + id_increase\n",
    "                    id_increase += 1\n",
    "                    occupancy_mask[mask]=1\n",
    "                    break\n",
    "        else:\n",
    "             mask = shorted_masks[i, 0, ...]\n",
    "             if (mask*occupancy_mask).sum()/mask.sum() <= overlapp_ratio:\n",
    "                mask[occupancy_mask]=0\n",
    "                seg_map[mask]=i+1\n",
    "                occupancy_mask[mask]=1\n",
    "        \n",
    "    # - project back to RGB\n",
    "    rgb_image = show_seg(img, seg_map)\n",
    "    return occupancy_mask, seg_map, rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Post Predict Segmentation\n",
    "def iter_post_predict_seg(\n",
    "    img: np.ndarray,\n",
    "    occupancy_mask,\n",
    "    seg_map,\n",
    "    input_points: np.ndarray = None,\n",
    "    point_labels: np.ndarray = None,\n",
    "    input_boxes: np.ndarray = None,\n",
    "    multimask_output: bool = False,\n",
    ") -> np.ndarray:\n",
    "    torch.autocast(device_type='cpu', dtype=torch.bfloat16).__enter__()\n",
    "    with torch.no_grad(): # prevent the net from caclulate gradient (more efficient inference)\n",
    "            predictor.set_image(img) # image encoder\n",
    "            masks, scores, logits = predictor.predict(  # prompt encoder + mask decoder\n",
    "                point_coords=input_points,\n",
    "                point_labels=point_labels,\n",
    "                box = input_boxes,\n",
    "                multimask_output=multimask_output)\n",
    "            \n",
    "    # - sort masks based on their scores (high-quality segmentation)\n",
    "    if len(masks.shape) == 3:\n",
    "        masks = masks.reshape((1, masks.shape[0], masks.shape[1], masks.shape[2]))\n",
    "        scores = scores.reshape((1, len(scores)))\n",
    "    shorted_masks = masks[np.argsort(scores[:,0])[::-1], :, :, :].astype(bool)\n",
    "    seg_map = seg_map\n",
    "    occupancy_mask = occupancy_mask\n",
    "    max_id = np.max(seg_map) + 1\n",
    "\n",
    "    id_increase = 0\n",
    "    # - add the masks one by one from high to low score\n",
    "    for i in range(shorted_masks.shape[0]):\n",
    "        if multimask_output:\n",
    "            mask_lst = shorted_masks[i]\n",
    "            score_lst = scores[i]\n",
    "            score_rank = np.argsort(score_lst)[::-1]\n",
    "            sorted_mask_lst = mask_lst[score_rank]\n",
    "            for mask in sorted_mask_lst:\n",
    "                if (mask*occupancy_mask).sum()/mask.sum() <= overlapp_ratio:\n",
    "                    mask = is_fully_connected(mask, kernel_size=30)\n",
    "                    mask = mask.astype(bool)\n",
    "                    mask[occupancy_mask]=0\n",
    "                    seg_map[mask]=max_id + id_increase\n",
    "                    id_increase += 1\n",
    "                    occupancy_mask[mask]=1\n",
    "                    break\n",
    "        else:\n",
    "             mask = shorted_masks[i, 0, ...]\n",
    "             if (mask*occupancy_mask).sum()/mask.sum() <= overlapp_ratio:\n",
    "                mask[occupancy_mask]=0\n",
    "                seg_map[mask]=max_id + i\n",
    "                occupancy_mask[mask]=1\n",
    "        \n",
    "    # - project back to RGB\n",
    "    rgb_image = show_seg(img, seg_map)\n",
    "    return occupancy_mask, seg_map, rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - temporary visualization\n",
    "def show_seg_v2(img, seg_map, coords=None, labels=None):\n",
    "    \"\"\"\n",
    "    Display segmentation map and original image with random RGB for each segment ID, using uint16 precision.\n",
    "    \n",
    "    Args:\n",
    "    - img: The original image (can be uint8 or uint16).\n",
    "    - seg_map: The segmentation map (each segment represented by an integer label).\n",
    "    - coords: Optional coordinates for annotations.\n",
    "    - labels: Optional labels for the annotation points.\n",
    "    \n",
    "    Returns:\n",
    "    - rgb_image: The RGB image generated from the segmentation map.\n",
    "    \"\"\"\n",
    "    # Create an empty RGB image using uint16\n",
    "    rgb_image = np.zeros((seg_map.shape[0], seg_map.shape[1], 3), dtype=np.uint16)\n",
    "\n",
    "    # Assign random RGB colors to each segment ID\n",
    "    for id_class in range(1, seg_map.max() + 1):\n",
    "        rgb_image[seg_map == id_class] = [np.random.randint(65535), np.random.randint(65535), np.random.randint(65535)]\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "\n",
    "    # First image (annotation)\n",
    "    if coords is not None and labels is not None:\n",
    "        plt.subplot(1, 3, 1)  # 1 row, 3 columns, first subplot\n",
    "        plt.imshow(rgb_image.astype(np.uint8))  # Downscale for visualization\n",
    "        show_points(coords, labels, plt.gca())\n",
    "        plt.title('Annotation')\n",
    "        plt.axis('off')  # Hide the axis\n",
    "    else:\n",
    "        plt.subplot(1, 3, 1)  # 1 row, 3 columns, first subplot\n",
    "        plt.imshow(rgb_image.astype(np.uint8))  # Downscale for visualization\n",
    "        plt.title('Annotation')\n",
    "        plt.axis('off')  # Hide the axis\n",
    "\n",
    "    # Second image (mix)\n",
    "    plt.subplot(1, 3, 2)  # 1 row, 3 columns, second subplot\n",
    "    # Downscale to uint8 for blending and display\n",
    "    img_uint8 = img.astype(np.uint8)\n",
    "    rgb_image_uint8 = (rgb_image / 256).astype(np.uint8)  # Scaling uint16 down to 0-255\n",
    "    mix_image = (rgb_image_uint8 / 2 + img_uint8 / 2).astype(np.uint8)  # Blend the images\n",
    "    plt.imshow(mix_image)\n",
    "    plt.title('Mix')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Third image (original image)\n",
    "    plt.subplot(1, 3, 3)  # 1 row, 3 columns, third subplot\n",
    "    plt.imshow(img_uint8)\n",
    "    plt.title('Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()  # Adjust the layout so everything fits without overlap\n",
    "    plt.show()\n",
    "\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = test_image_path_lst[0]\n",
    "num_labels, bboxes, image = flo_infer_bboxes(image_path, 2048, num_steps=1)\n",
    "bboxes = validate_boxes(bboxes)\n",
    "img = cv2.imread(str(image_path))\n",
    "occupancy_mask, seg_map, rgb_image = iter_predict_seg( img , input_boxes = bboxes, multimask_output = True)\n",
    "print(f\"The initial number of labels: {len(np.unique(seg_map))}\")\n",
    "pre = len(np.unique(seg_map))\n",
    "count = 0\n",
    "for i in tqdm(range(100)):\n",
    "    num_labels, bboxes, image = flo_infer_bboxes(image_path, 2048, num_steps=1)\n",
    "    occupancy_mask, seg_map, rgb_image = iter_predict_seg( img , occupancy_mask, seg_map, input_boxes = bboxes, multimask_output = True)\n",
    "    cur = len(np.unique(seg_map))\n",
    "    # if (i + 1) % 5 == 0:\n",
    "    #     show_seg_v2(img, seg_map)\n",
    "    print(f\"The updating number of labels: {cur}\")\n",
    "    if count == 20:\n",
    "        break\n",
    "    if cur == pre:\n",
    "        count += 1\n",
    "    else:\n",
    "        pre = cur\n",
    "        count = 0\n",
    "rgb_iamge = show_seg_v2(img, seg_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_stage_occupancy_mask = occupancy_mask.copy()\n",
    "first_stage_seg_map = seg_map.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_mask = first_stage_occupancy_mask.copy()\n",
    "seg_map = first_stage_seg_map.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_seg_v2(img, seg_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "pre = len(np.unique(seg_map))\n",
    "for i in tqdm(range(10)):\n",
    "    num_labels, bboxes, image = flo_infer_bboxes(image_path, 2048, num_steps=1)\n",
    "    occupancy_mask, seg_map, rgb_image = iter_predict_seg( img , occupancy_mask, seg_map, input_boxes = bboxes, multimask_output = True, filter = False)\n",
    "    cur = len(np.unique(seg_map))\n",
    "    if (i + 1) % 5:\n",
    "        show_seg_v2(img, seg_map)\n",
    "    print(f\"The updating number of labels: {cur}\")\n",
    "    if count == 20:\n",
    "        break\n",
    "    if cur == pre:\n",
    "        count += 1\n",
    "    else:\n",
    "        pre = cur\n",
    "        count = 0\n",
    "    \n",
    "\n",
    "print(f\"the final number of labels: {len(np.unique(seg_map))}\")\n",
    "show_seg_v2(img, seg_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_count = 0\n",
    "for j in tqdm(range(8, 1, -1)):\n",
    "    for k in tqdm(range(20), leave = False):\n",
    "        erode_size = (j, j)\n",
    "        false_coordinates = np.array(select_sparse_coordinates(occupancy_mask, erode_kernel_size=erode_size, dilate_kernel_size = erode_size))\n",
    "        if false_coordinates.shape[0] == 0:\n",
    "            continue\n",
    "        false_labels = np.ones([false_coordinates.shape[0],])\n",
    "        # reshape coordinates and prepare labels\n",
    "        if (k + 1) % 10 == 0:\n",
    "            show_seg_v2(img, seg_map, false_coordinates, false_labels)\n",
    "        false_coordinates = false_coordinates.reshape((false_coordinates.shape[0], 1, false_coordinates.shape[1]))\n",
    "        false_labels = np.ones([false_coordinates.shape[0], 1])\n",
    "        occupancy_mask, seg_map, rgb_image = iter_post_predict_seg( img, occupancy_mask, seg_map, input_points = false_coordinates, point_labels = false_labels, multimask_output=True)\n",
    "        print(f\"The updating number of labels: {len(np.unique(seg_map))}\")\n",
    "    temp_count += 1\n",
    "show_seg_v2(img, seg_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_coordinates = np.array(select_sparse_coordinates(occupancy_mask, erode_kernel_size=erode_size))\n",
    "false_coordinates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_coordinates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_kernel = np.ones((10, 10), np.uint8)\n",
    "more_smoothed_image = cv2.morphologyEx(seg_map, cv2.MORPH_CLOSE, larger_kernel)\n",
    "show_seg_v2(img, more_smoothed_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuxuan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
