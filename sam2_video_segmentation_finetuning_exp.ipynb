{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM Video Segmentation Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torchvision.ops import masks_to_boxes\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Global Variables\n",
    "\n",
    "CHECKPOINT = \"./sam2_hiera_large.pt\"\n",
    "CONFIG = \"sam2_hiera_l.yaml\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "\n",
    "sam2_model = build_sam2_video_predictor(CONFIG, CHECKPOINT, device=DEVICE)\n",
    "\n",
    "log_dir = 'logs'\n",
    "\n",
    "exp_name = 'sam_video_finetune'\n",
    "\n",
    "data_dir = Path('./snemi')\n",
    "raw_image_dir = data_dir / 'image_jpgs'\n",
    "seg_dir = data_dir / 'seg_jpgs'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Utils Function\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import dateutil.tz\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "\n",
    "def set_log_dir(root_dir, exp_name):\n",
    "    path_dict = {}\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "    # set log path\n",
    "    exp_path = os.path.join(root_dir, exp_name)\n",
    "    now = datetime.now(dateutil.tz.tzlocal())\n",
    "    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "    prefix = exp_path + '_' + timestamp\n",
    "    os.makedirs(prefix)\n",
    "    path_dict['prefix'] = prefix\n",
    "\n",
    "    # set checkpoint path\n",
    "    ckpt_path = os.path.join(prefix, 'Model')\n",
    "    os.makedirs(ckpt_path)\n",
    "    path_dict['ckpt_path'] = ckpt_path\n",
    "\n",
    "    log_path = os.path.join(prefix, 'Log')\n",
    "    os.makedirs(log_path)\n",
    "    path_dict['log_path'] = log_path\n",
    "\n",
    "    # set sample image path for fid calculation\n",
    "    sample_path = os.path.join(prefix, 'Samples')\n",
    "    os.makedirs(sample_path)\n",
    "    path_dict['sample_path'] = sample_path\n",
    "\n",
    "    return path_dict\n",
    "\n",
    "\n",
    "\n",
    "def create_logger(log_dir, phase='train'):\n",
    "    time_str = time.strftime('%Y-%m-%d-%H-%M')\n",
    "    log_file = '{}_{}.log'.format(time_str, phase)\n",
    "    final_log_file = os.path.join(log_dir, log_file)\n",
    "    head = '%(asctime)-15s %(message)s'\n",
    "    logging.basicConfig(filename=str(final_log_file),\n",
    "                        format=head)\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    console = logging.StreamHandler()\n",
    "    logging.getLogger('').addHandler(console)\n",
    "\n",
    "    return logger\n",
    "path_helper = set_log_dir(log_dir, exp_name)\n",
    "logger = create_logger(path_helper['log_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAM2VideoPredictor(\n",
       "  (image_encoder): ImageEncoder(\n",
       "    (trunk): Hiera(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 144, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x MultiScaleBlock(\n",
       "          (norm1): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
       "            (proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=144, out_features=576, bias=True)\n",
       "              (1): Linear(in_features=576, out_features=144, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (2): MultiScaleBlock(\n",
       "          (norm1): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n",
       "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "            (qkv): Linear(in_features=144, out_features=864, bias=True)\n",
       "            (proj): Linear(in_features=288, out_features=288, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=288, out_features=1152, bias=True)\n",
       "              (1): Linear(in_features=1152, out_features=288, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "          (proj): Linear(in_features=144, out_features=288, bias=True)\n",
       "        )\n",
       "        (3-7): 5 x MultiScaleBlock(\n",
       "          (norm1): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (qkv): Linear(in_features=288, out_features=864, bias=True)\n",
       "            (proj): Linear(in_features=288, out_features=288, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=288, out_features=1152, bias=True)\n",
       "              (1): Linear(in_features=1152, out_features=288, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (8): MultiScaleBlock(\n",
       "          (norm1): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n",
       "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "            (qkv): Linear(in_features=288, out_features=1728, bias=True)\n",
       "            (proj): Linear(in_features=576, out_features=576, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=576, out_features=2304, bias=True)\n",
       "              (1): Linear(in_features=2304, out_features=576, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "          (proj): Linear(in_features=288, out_features=576, bias=True)\n",
       "        )\n",
       "        (9-43): 35 x MultiScaleBlock(\n",
       "          (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (qkv): Linear(in_features=576, out_features=1728, bias=True)\n",
       "            (proj): Linear(in_features=576, out_features=576, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=576, out_features=2304, bias=True)\n",
       "              (1): Linear(in_features=2304, out_features=576, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (44): MultiScaleBlock(\n",
       "          (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "            (qkv): Linear(in_features=576, out_features=3456, bias=True)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=1152, out_features=4608, bias=True)\n",
       "              (1): Linear(in_features=4608, out_features=1152, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "          (proj): Linear(in_features=576, out_features=1152, bias=True)\n",
       "        )\n",
       "        (45-47): 3 x MultiScaleBlock(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=1152, out_features=4608, bias=True)\n",
       "              (1): Linear(in_features=4608, out_features=1152, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): FpnNeck(\n",
       "      (position_encoding): PositionEmbeddingSine()\n",
       "      (convs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (conv): Conv2d(1152, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (conv): Conv2d(576, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (conv): Conv2d(288, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (conv): Conv2d(144, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mask_downsample): Conv2d(1, 1, kernel_size=(4, 4), stride=(4, 4))\n",
       "  (memory_attention): MemoryAttention(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x MemoryAttentionLayer(\n",
       "        (self_attn): RoPEAttention(\n",
       "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (cross_attn_image): RoPEAttention(\n",
       "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_proj): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (memory_encoder): MemoryEncoder(\n",
       "    (mask_downsampler): MaskDownSampler(\n",
       "      (encoder): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (7): LayerNorm2d()\n",
       "        (8): GELU(approximate='none')\n",
       "        (9): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (10): LayerNorm2d()\n",
       "        (11): GELU(approximate='none')\n",
       "        (12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (pix_feat_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fuser): Fuser(\n",
       "      (proj): Identity()\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x CXBlock(\n",
       "          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "          (norm): LayerNorm2d()\n",
       "          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (position_encoding): PositionEmbeddingSine()\n",
       "    (out_proj): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (sam_prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (sam_mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (1): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            )\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (obj_score_token): Embedding(1, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (conv_s0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv_s1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "        (act): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (pred_obj_score_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (act): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (obj_ptr_proj): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (act): ReLU()\n",
       "  )\n",
       "  (obj_ptr_tpos_proj): Identity()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - Print Out Sam2 Model Architecutre\n",
    "sam2_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Load Layer Parameters\n",
    "sam_layers = ([]\n",
    "            # + list(sam2_model.image_encoder.parameters())\n",
    "            + list(sam2_model.sam_prompt_encoder.parameters())\n",
    "            + list(sam2_model.sam_mask_decoder.parameters())\n",
    "            )\n",
    "mem_layers = ([]\n",
    "            + list(sam2_model.obj_ptr_proj.parameters())\n",
    "            + list(sam2_model.memory_encoder.parameters())\n",
    "            + list(sam2_model.memory_attention.parameters())\n",
    "            + list(sam2_model.mask_downsample.parameters())\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Define Optimizers\n",
    "if len(sam_layers) == 0:\n",
    "    optimizer1 = None\n",
    "else:\n",
    "    optimizer1 = optim.Adam(sam_layers, lr=1e-4, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "if len(mem_layers) == 0:\n",
    "    optimizer2 = None\n",
    "else:\n",
    "    optimizer2 = optim.Adam(mem_layers, lr=1e-8, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float Quantization\n",
    "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "if torch.cuda.get_device_properties(0).major >= 8:\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img shape: (1024, 1024, 3)\n",
      "mask_arr shape: (255, 1024, 1024, 3)\n",
      "point_arr shape: (255, 1, 2)\n",
      "input_boxes shape: (255, 4)\n",
      "point_labels shape: (255, 1)\n"
     ]
    }
   ],
   "source": [
    "# - Load Data Mannually\n",
    "img_file_path_lst = sorted([raw_image_dir / img_file for img_file in os.listdir(raw_image_dir)])\n",
    "seg_file_path_lst = sorted([seg_dir / img_file for img_file in os.listdir(seg_dir)])\n",
    "data = []\n",
    "for i in range(len(img_file_path_lst)):\n",
    "    data.append({'image': img_file_path_lst[i], 'annotation': seg_file_path_lst[i]})\n",
    "# - split train dataset and validation dataset\n",
    "valid_data = data[80:]\n",
    "data = data[:80]\n",
    "# - read batch for jpg files \n",
    "def read_frame(data, idx):\n",
    "     #  select image\n",
    "     ent  = data[idx] # choose random entry\n",
    "     Img = cv2.imread(str(ent[\"image\"])) # read image\n",
    "     ann_map_grayscale = np.array(Image.open(ent['annotation']))\n",
    "     ann_map = ann_map_grayscale\n",
    "     ann_map_grayscale = ann_map[...,0]\n",
    "     \n",
    "     if Img.shape[0] > 1024 or Img.shape[1] > 1024:\n",
    "          # Calculate scaling factor\n",
    "          r = np.min([1024 / Img.shape[1], 1024 / Img.shape[0]])  # Scaling factor to fit within 1024x1024\n",
    "          # Resize the image\n",
    "          Img = cv2.resize(Img, (int(Img.shape[1] * r), int(Img.shape[0] * r)))\n",
    "          # Resize the annotation map (with nearest neighbor interpolation)\n",
    "          ann_map = cv2.resize(ann_map, (int(ann_map.shape[1] * r), int(ann_map.shape[0] * r)), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "     # - get bounding box\n",
    "     inds = np.unique(ann_map_grayscale)[1:] # load all indices\n",
    "\n",
    "     masks = [] \n",
    "     for ind in inds:\n",
    "        masks.append(ann_map_grayscale == ind)\n",
    "     masks = np.array(masks)\n",
    "     masks_tensor = torch.from_numpy(masks)\n",
    "\n",
    "     boxes = masks_to_boxes(masks_tensor)\n",
    "     input_boxes = boxes.numpy()\n",
    "\n",
    "\n",
    "\n",
    "     # Get binary masks and points\n",
    "     mat_map = ann_map\n",
    "     inds = np.unique(mat_map)[1:] # load all indices\n",
    "     points= []\n",
    "     masks = [] \n",
    "     for ind in inds:\n",
    "          mask=(mat_map == ind).astype(np.uint8) # make binary mask\n",
    "          masks.append(mask)\n",
    "          coords = np.argwhere(mask > 0) # get all coordinates in mask\n",
    "          yx = np.array(coords[np.random.randint(len(coords))]) # choose random point/coordinate\n",
    "          points.append([[yx[1], yx[0]]])\n",
    "     return Img,np.array(masks),np.array(points), input_boxes, np.ones([len(masks),1])\n",
    "img, mask_arr,  point_arr, input_boxes, one_arr= read_frame(data, 0)\n",
    "print(f'img shape: {img.shape}')\n",
    "print(f'mask_arr shape: {mask_arr.shape}')\n",
    "print(f'point_arr shape: {point_arr.shape}')\n",
    "print(f'input_boxes shape: {input_boxes.shape}')\n",
    "print(f'point_labels shape: {one_arr.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # - Load Data only for bbox, point labels; return one frame info. If there are multiple videos, we need to modify the dataloader\n",
    "# from torch.utils.data import Dataset\n",
    "# class SnemiDataset(Dataset):\n",
    "#     def __init__(self, img_files, ann_files):\n",
    "#         self.img_files = img_files\n",
    "#         self.ann_files = ann_files\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.img_files)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         img_file_name = str(img_files[idx])\n",
    "#         ann_file_name = str(img_files[idx])\n",
    "#         mask_dict = {}\n",
    "#         point_label_dict = {}\n",
    "#         pt_dict = {}\n",
    "#         image_meta_dict = {}\n",
    "\n",
    "#         # - read image and segmentation\n",
    "#         img = cv2.imread(img_file_name) # read image\n",
    "#         img_tensor = torch.tensor(img).permutae(2, 0, 1)\n",
    "#         ann_map_grayscale = np.array(Image.open(ann_file_name))\n",
    "#         ann_map = np.stack((ann_map_grayscale, ) * 3, axis = -1)\n",
    "\n",
    "#         ids = np.unique(ann_map)[1:]\n",
    "\n",
    "#         for id in ids:\n",
    "#             mask = ann_map_grayscale == id\n",
    "#             mask_dict[id] = torch.from_numpy(mask)\n",
    "\n",
    "\n",
    "\n",
    "#         return {\n",
    "#                 'image':img_tensor,\n",
    "#                 'label': mask_dict,\n",
    "#                 'p_label':point_label_dict,\n",
    "#                 'pt':pt_dict,\n",
    "#                 'image_meta_dict':image_meta_dict,\n",
    "#             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Begin Training\n",
    "best_acc = 0.0\n",
    "best_tol = 1e4\n",
    "best_dice = 0.0\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Start Training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    sam2_model.train()\n",
    "    time_start = time.time()\n",
    "    loss, prompt_loss, non_prompt_loss = function.train_sam(args, net, optimizer1, optimizer2, nice_train_loader, epoch)\n",
    "    logger.info(f'Train loss: {loss}, {prompt_loss}, {non_prompt_loss} || @ epoch {epoch}.')\n",
    "    time_end = time.time()\n",
    "    print('time_for_training ', time_end - time_start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuxuan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
