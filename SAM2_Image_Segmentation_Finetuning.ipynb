{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM2 Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.ops import masks_to_boxes\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAM2 Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Global Variables\n",
    "\n",
    "data_dir= Path(\"./snemi/\" )\n",
    "raw_image_dir = data_dir / 'image_pngs'\n",
    "seg_image_dir = data_dir / 'seg_pngs'\n",
    "\n",
    "sam2_checkpoint = \"./sam2_hiera_large.pt\"\n",
    "model_cfg = \"./sam2_hiera_l.yaml\"\n",
    "itrs = 10000\n",
    "\n",
    "checkpoint_dir = './checkpoints/all'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs('./checkpoints/all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Prepare dataset\n",
    "data = []\n",
    "for ff, name in enumerate(os.listdir(raw_image_dir)):\n",
    "    data.append({'image': raw_image_dir / f'image{ff:04d}.png', 'annotation': seg_image_dir / f'seg{ff:04d}.png'})\n",
    "# - split train dataset and validation dataset\n",
    "valid_data = data[80:]\n",
    "data = data[:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Read Data\n",
    "\n",
    "def read_batch(data):\n",
    "     #  select image\n",
    "     ent  = data[np.random.randint(len(data))] # choose random entry\n",
    "     Img = cv2.imread(str(ent[\"image\"])) # read image\n",
    "     ann_map_grayscale = np.array(Image.open(ent['annotation']))\n",
    "     ann_map = np.stack((ann_map_grayscale, ) * 3, axis = -1)\n",
    "     print(f'image shape: {np.array(Img).shape}')\n",
    "     print(f'annotation shape: {np.array(ann_map).shape}')\n",
    "     \n",
    "     # resize image\n",
    "     r = np.min([1024 / Img.shape[1], 1024 / Img.shape[0]]) # scalling factor\n",
    "     Img = cv2.resize(Img, (int(Img.shape[1] * r), int(Img.shape[0] * r)))\n",
    "     ann_map = cv2.resize(ann_map, (int(ann_map.shape[1] * r), int(ann_map.shape[0] * r)),interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "     # - get bounding box\n",
    "     inds = np.unique(ann_map_grayscale)[1:] # load all indices\n",
    "\n",
    "     masks = [] \n",
    "     for ind in inds:\n",
    "        masks.append(ann_map_grayscale == ind)\n",
    "     masks = np.array(masks)\n",
    "     masks_tensor = torch.from_numpy(masks)\n",
    "\n",
    "     boxes = masks_to_boxes(masks_tensor)\n",
    "     input_boxes = boxes.numpy()\n",
    "\n",
    "\n",
    "\n",
    "     # Get binary masks and points\n",
    "     mat_map = ann_map\n",
    "     inds = np.unique(mat_map)[1:] # load all indices\n",
    "     points= []\n",
    "     masks = [] \n",
    "     for ind in inds:\n",
    "          mask=(mat_map == ind).astype(np.uint8) # make binary mask\n",
    "          masks.append(mask)\n",
    "          coords = np.argwhere(mask > 0) # get all coordinates in mask\n",
    "          yx = np.array(coords[np.random.randint(len(coords))]) # choose random point/coordinate\n",
    "          points.append([[yx[1], yx[0]]])\n",
    "     return Img,np.array(masks),np.array(points), input_boxes, np.ones([len(masks),1])\n",
    "img, mask_arr,  point_arr, input_boxes, one_arr= read_batch(data)\n",
    "print(f'mask_arr shape: {mask_arr.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_masks_opencv(mask, output_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Reshapes the input NumPy array by selecting the first of the 3 redundant channels,\n",
    "    then resizes each mask to the given output size using nearest-neighbor interpolation.\n",
    "\n",
    "    Args:\n",
    "    - mask (np.ndarray): Array of shape (223, 1024, 1024, 3).\n",
    "    - output_size (tuple): Desired output size (H, W) for the mask. Default is (256, 256).\n",
    "\n",
    "    Returns:\n",
    "    - resized_masks: Resized masks of shape (223, 1, 256, 256).\n",
    "    \"\"\"\n",
    "    # Select the first channel (shape becomes (223, 1024, 1024))\n",
    "    masks_single_channel = mask[..., 0]\n",
    "    \n",
    "    # Reshape to (223, 1, 1024, 1024) to add the single channel back using np.expand_dims\n",
    "    reshaped_masks = np.expand_dims(masks_single_channel, axis=1)\n",
    "\n",
    "    # Initialize the array to store resized masks (223, 1, 256, 256)\n",
    "    resized_masks = np.zeros((reshaped_masks.shape[0], 1, output_size[0], output_size[1]), dtype=reshaped_masks.dtype)\n",
    "    \n",
    "    # Loop over each mask and resize using cv2.resize with nearest-neighbor interpolation\n",
    "    for i in range(reshaped_masks.shape[0]):\n",
    "        resized_masks[i, 0, :, :] = cv2.resize(reshaped_masks[i, 0, :, :], output_size, interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    return resized_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Finetuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Build Model\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device='cpu')\n",
    "predictor = SAM2ImagePredictor(sam2_model)\n",
    "# - Set Training Parameters\n",
    "predictor.model.sam_mask_decoder.train(True)\n",
    "predictor.model.sam_prompt_encoder.train(True)\n",
    "predictor.model.image_encoder.train(True)\n",
    "# - Set Optimizer and Scaler\n",
    "optimizer=torch.optim.AdamW(params=predictor.model.parameters(),lr=1e-5,weight_decay=4e-5)\n",
    "scaler = torch.amp.GradScaler()\n",
    "# - Force cuda operations to be synchronized\n",
    "!export CUDA_LAUNCH_BLOCKING=1\n",
    "# - Define Devices\n",
    "device0 = torch.device('cuda:0')\n",
    "device1 = torch.device('cuda:1')\n",
    "device_cpu = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Finetuning by Using Points, Boxes, Mask Logits\n",
    "for itr in range(itrs):\n",
    "    with torch.amp.autocast(device_type='cpu'):\n",
    "        image, mask, input_point, input_boxes, input_label = read_batch(data)\n",
    "        if mask.shape[0] == 0: continue\n",
    "        predictor.set_image(image)\n",
    "\n",
    "        reshaped_masks = resize_masks_opencv(mask)\n",
    "\n",
    "\n",
    "        # - prompt encoding\n",
    "        mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(input_point, input_label, box=input_boxes, mask_logits=None, normalize_coords=True)\n",
    "        sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(points=(unnorm_coords, labels), boxes=unnorm_box, masks=mask_input)\n",
    "\n",
    "\n",
    "\n",
    "        # - mask decoder\n",
    "        batched_mode = unnorm_coords.shape[0] > 1\n",
    "        high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n",
    "        low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n",
    "            image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),\n",
    "            image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=True,\n",
    "            repeat_image=batched_mode,\n",
    "            high_res_features=[feat for feat in high_res_features],\n",
    "        )\n",
    "\n",
    "\n",
    "        prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])\n",
    "\n",
    "        # - segmentation loss calculation on CPU\n",
    "        gt_mask = torch.tensor(mask.astype(np.float32))[:, :, :, 0]\n",
    "        prd_mask = torch.sigmoid(prd_masks[:, 0])\n",
    "        seg_loss = (-gt_mask * torch.log(prd_mask + 0.00001) - (1 - gt_mask) * torch.log((1 - prd_mask) + 0.00001)).mean()\n",
    "\n",
    "        # - score loss calculation (intersection over union) IOU on CPU\n",
    "        inter = (gt_mask * (prd_mask > 0.5)).sum(1).sum(1)\n",
    "        iou = inter / (gt_mask.sum(1).sum(1) + (prd_mask > 0.5).sum(1).sum(1) - inter)\n",
    "        score_loss = torch.abs(prd_scores[:, 0] - iou).mean()\n",
    "\n",
    "        # Move the total loss to GPU for backpropagation\n",
    "        loss = (seg_loss + score_loss * 0.05)\n",
    "        predictor.model.zero_grad()  # empty gradient\n",
    "        scaler.scale(loss).backward()  # Backpropagate\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()  # Mix precision\n",
    "\n",
    "        if (itr + 1) % 1000 == 0:\n",
    "            torch.save(predictor.model.state_dict(), f\"./checkpoints/all/large_model_all_{itr}.torch\")\n",
    "            print(\"save model\")\n",
    "\n",
    "        if itr == 0:\n",
    "            mean_iou = 0\n",
    "        mean_iou = mean_iou * 0.99 + 0.01 * np.mean(iou.cpu().detach().numpy())\n",
    "        print(\"step)\", itr, \"Accuracy(IOU)=\", mean_iou)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuxuan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
