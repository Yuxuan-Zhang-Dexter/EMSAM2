{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM2 Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.ops import masks_to_boxes\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAM2 Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Global Variables\n",
    "\n",
    "data_dir= Path(\"./snemi/\" )\n",
    "raw_image_dir = data_dir / 'image_pngs'\n",
    "seg_image_dir = data_dir / 'seg_pngs'\n",
    "raw_image_slice_dir = data_dir / 'image_slice_pngs'\n",
    "seg_image_slice_dir = data_dir / 'seg_slice_pngs'\n",
    "test_iamge_slice_dir = data_dir / \"image_slice_test_pngs\"\n",
    "\n",
    "sam2_checkpoint = \"./sam2_hiera_large.pt\"\n",
    "model_cfg = \"./sam2_hiera_l.yaml\"\n",
    "itrs = 10000\n",
    "log_dir=\"./logs\"\n",
    "val_num = 10\n",
    "\n",
    "checkpoint_dir = './checkpoints/all'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs('./checkpoints/all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Prepare dataset\n",
    "data = []\n",
    "for ff, name in enumerate(os.listdir(raw_image_dir)):\n",
    "    data.append({'image': raw_image_dir / f'image{ff:04d}.png', 'annotation': seg_image_dir / f'seg{ff:04d}.png'})\n",
    "# - split train dataset and validation dataset\n",
    "valid_data = data[80:]\n",
    "data = data[:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - slice image and segmentation for florence sequence length limit\n",
    "def create_slices(image_path, slice_image_dir):\n",
    "    img = Image.open(image_path)\n",
    "     # Get image dimensions\n",
    "    width, height = img.size\n",
    "    \n",
    "    # Calculate the midpoint\n",
    "    mid_x, mid_y = width // 2, height // 2\n",
    "    \n",
    "    # Define the four slices (left, upper, right, lower)\n",
    "    slices = {\n",
    "        'top_left': (0, 0, mid_x, mid_y),\n",
    "        'top_right': (mid_x, 0, width, mid_y),\n",
    "        'bottom_left': (0, mid_y, mid_x, height),\n",
    "        'bottom_right': (mid_x, mid_y, width, height)\n",
    "    }\n",
    "    \n",
    "    # Loop through the slices, crop, and save them\n",
    "    all_slices = []\n",
    "    for key, coords in slices.items():\n",
    "        slice_img = img.crop(coords)\n",
    "        # Format the name: base name + coordinates\n",
    "        slice_filename = f\"{image_path.stem}_{coords[0]}_{coords[1]}_{coords[2]}_{coords[3]}.png\"\n",
    "        slice_img.save( slice_image_dir / slice_filename)\n",
    "        all_slices.append( slice_image_dir / slice_filename)\n",
    "\n",
    "    return all_slices\n",
    "\n",
    "def slice_all_image_seg(data, raw_image_slice_dir, seg_image_slice_dir):\n",
    "    new_data = []\n",
    "    for element in data:\n",
    "        image_path = element['image']\n",
    "        seg_path = element['annotation']\n",
    "        image_lst = create_slices(image_path, raw_image_slice_dir)\n",
    "        seg_lst = create_slices(seg_path, seg_image_slice_dir)\n",
    "\n",
    "        for i in range(len(image_lst)):\n",
    "            new_data.append({'image': image_lst[i], 'annotation': seg_lst[i]})\n",
    "        \n",
    "    return new_data\n",
    "\n",
    "data = slice_all_image_seg(data, raw_image_slice_dir, seg_image_slice_dir)\n",
    "valid_data = slice_all_image_seg(valid_data, raw_image_slice_dir, seg_image_slice_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_arr shape: (64, 512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "# - Read Data\n",
    "\n",
    "def read_batch(data):\n",
    "     #  select image\n",
    "     ent  = data[np.random.randint(len(data))] # choose random entry\n",
    "     Img = cv2.imread(str(ent[\"image\"])) # read image\n",
    "     ann_map_grayscale = np.array(Image.open(ent['annotation']))\n",
    "     ann_map = np.stack((ann_map_grayscale, ) * 3, axis = -1)\n",
    "     \n",
    "     if Img.shape[0] > 1024 or Img.shape[1] > 1024:\n",
    "          # Calculate scaling factor\n",
    "          r = np.min([1024 / Img.shape[1], 1024 / Img.shape[0]])  # Scaling factor to fit within 1024x1024\n",
    "          # Resize the image\n",
    "          Img = cv2.resize(Img, (int(Img.shape[1] * r), int(Img.shape[0] * r)))\n",
    "          # Resize the annotation map (with nearest neighbor interpolation)\n",
    "          ann_map = cv2.resize(ann_map, (int(ann_map.shape[1] * r), int(ann_map.shape[0] * r)), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "     # - get bounding box\n",
    "     inds = np.unique(ann_map_grayscale)[1:] # load all indices\n",
    "\n",
    "     masks = [] \n",
    "     for ind in inds:\n",
    "        masks.append(ann_map_grayscale == ind)\n",
    "     masks = np.array(masks)\n",
    "     masks_tensor = torch.from_numpy(masks)\n",
    "\n",
    "     boxes = masks_to_boxes(masks_tensor)\n",
    "     input_boxes = boxes.numpy()\n",
    "\n",
    "\n",
    "\n",
    "     # Get binary masks and points\n",
    "     mat_map = ann_map\n",
    "     inds = np.unique(mat_map)[1:] # load all indices\n",
    "     points= []\n",
    "     masks = [] \n",
    "     for ind in inds:\n",
    "          mask=(mat_map == ind).astype(np.uint8) # make binary mask\n",
    "          masks.append(mask)\n",
    "          coords = np.argwhere(mask > 0) # get all coordinates in mask\n",
    "          yx = np.array(coords[np.random.randint(len(coords))]) # choose random point/coordinate\n",
    "          points.append([[yx[1], yx[0]]])\n",
    "     return Img,np.array(masks),np.array(points), input_boxes, np.ones([len(masks),1])\n",
    "img, mask_arr,  point_arr, input_boxes, one_arr= read_batch(data)\n",
    "print(f'mask_arr shape: {mask_arr.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_logits(mask, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Convert binary mask to mask logits.\n",
    "\n",
    "    Args:\n",
    "        mask (torch.Tensor or np.ndarray): A binary mask tensor with values in {0, 1}.\n",
    "        epsilon (float): A small value to prevent division by zero.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The corresponding logits.\n",
    "    \"\"\"\n",
    "    # Ensure the mask is in float32 and has values in range [0, 1]\n",
    "    mask = mask.astype(np.float32)\n",
    "    \n",
    "    # Apply the logit function: log(p / (1 - p))\n",
    "    logits = np.log(mask + epsilon) - np.log(1 - mask + epsilon)\n",
    "    \n",
    "    return logits\n",
    "\n",
    "\n",
    "def resize_masks_opencv(mask, output_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Reshapes the input NumPy array by selecting the first of the 3 redundant channels,\n",
    "    then resizes each mask to the given output size using nearest-neighbor interpolation.\n",
    "    After resizing, the binary masks (0 and 1) are converted to logits.\n",
    "\n",
    "    Args:\n",
    "    - mask (np.ndarray): Array of shape (223, 1024, 1024, 3).\n",
    "    - output_size (tuple): Desired output size (H, W) for the mask. Default is (256, 256).\n",
    "\n",
    "    Returns:\n",
    "    - resized_mask_logits: Resized mask logits of shape (223, 1, 256, 256).\n",
    "    \"\"\"\n",
    "    # Select the first channel (shape becomes (223, 1024, 1024))\n",
    "    masks_single_channel = mask[..., 0]\n",
    "    \n",
    "    # Reshape to (223, 1, 1024, 1024) to add the single channel back using np.expand_dims\n",
    "    reshaped_masks = np.expand_dims(masks_single_channel, axis=1)\n",
    "\n",
    "    # Initialize the array to store resized masks (223, 1, 256, 256)\n",
    "    resized_masks = np.zeros((reshaped_masks.shape[0], 1, output_size[0], output_size[1]), dtype=reshaped_masks.dtype)\n",
    "    \n",
    "    # Loop over each mask and resize using cv2.resize with nearest-neighbor interpolation\n",
    "    for i in range(reshaped_masks.shape[0]):\n",
    "        resized_masks[i, 0, :, :] = cv2.resize(reshaped_masks[i, 0, :, :], output_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Convert the resized binary masks (0 and 1) to mask logits\n",
    "    resized_mask_logits = np.zeros_like(resized_masks, dtype=np.float32)\n",
    "    for i in range(resized_masks.shape[0]):\n",
    "        resized_mask_logits[i, 0, :, :] = mask_to_logits(resized_masks[i, 0, :, :])\n",
    "    \n",
    "    return resized_mask_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Finetuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Build Model\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device='cpu')\n",
    "predictor = SAM2ImagePredictor(sam2_model)\n",
    "# - Set Training Parameters\n",
    "predictor.model.sam_mask_decoder.train(True)\n",
    "predictor.model.sam_prompt_encoder.train(True)\n",
    "predictor.model.image_encoder.train(True)\n",
    "# - Set Optimizer and Scaler\n",
    "optimizer=torch.optim.AdamW(params=predictor.model.parameters(),lr=1e-5,weight_decay=4e-5)\n",
    "scaler = torch.amp.GradScaler()\n",
    "# - Force cuda operations to be synchronized\n",
    "!export CUDA_LAUNCH_BLOCKING=1\n",
    "# - Define Devices\n",
    "device0 = torch.device('cuda:0')\n",
    "device1 = torch.device('cuda:1')\n",
    "device_cpu = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IOU at iteration 1: 0.008890880346298218\n",
      "tensor([0.2999, 0.6136, 0.9041, 0.8043, 0.8168, 0.9331, 0.9613, 0.2648, 0.9659,\n",
      "        0.8611, 0.9293, 0.8414, 0.8593, 0.9043, 0.9238, 0.9344, 0.7657, 0.8494,\n",
      "        0.8800, 0.8904, 0.8756, 0.7221, 0.8314, 0.9111, 0.9258, 0.9310, 0.8420,\n",
      "        0.8553, 0.8519, 0.8424, 0.8743, 0.8981, 0.9190, 0.7528, 0.8654, 0.8700,\n",
      "        0.9080, 0.8211, 0.9126, 0.8986, 0.8508, 0.7543, 0.8369, 0.9035, 0.6735,\n",
      "        0.2425, 0.8741, 0.8169, 0.7463, 0.8405, 0.8963, 0.8719, 0.8676, 0.1012,\n",
      "        0.3604, 0.9195, 0.7761, 0.8594, 0.9032, 0.8063, 0.8484, 0.8423, 0.8312,\n",
      "        0.9064, 0.8433, 0.8223, 0.7826, 0.6420, 0.7427, 0.9013])\n",
      "tensor([0.9381, 0.8797, 0.8792, 0.5185, 0.5893, 0.8863, 0.8253, 0.7427, 0.9185,\n",
      "        0.9401, 0.9062, 0.7980, 0.8295, 0.9360, 0.8190, 0.8059, 0.8022, 0.8744,\n",
      "        0.8075, 0.8983, 0.9080, 0.8226, 0.7326, 0.6758, 0.8886, 0.8895, 0.0500,\n",
      "        0.9188, 0.7467, 0.8704, 0.8053, 0.8687, 0.9171, 0.8401, 0.8514, 0.9018,\n",
      "        0.8366, 0.8941, 0.8295, 0.8935, 0.8811, 0.8986, 0.8450, 0.7940, 0.8431,\n",
      "        0.8621, 0.7553, 0.8906, 0.8045, 0.8023, 0.8230, 0.8402, 0.7588, 0.8094,\n",
      "        0.8041, 0.9211, 0.8758, 0.8737, 0.8442, 0.7310, 0.9041, 0.9105, 0.7459,\n",
      "        0.7185, 0.7699, 0.8134, 0.8437, 0.8257])\n",
      "tensor([0.9094, 0.0098, 0.7789, 0.9157, 0.8580, 0.8305, 0.6753, 0.9057, 0.9059,\n",
      "        0.6509, 0.8011, 0.8068, 0.7574, 0.8998, 0.8434, 0.8016, 0.3696, 0.9001,\n",
      "        0.8495, 0.8745, 0.9537, 0.7866, 0.9564, 0.7743, 0.8724, 0.8539, 0.8939,\n",
      "        0.5461, 0.8560, 0.9047, 0.8351, 0.8010, 0.9123, 0.8203, 0.8807, 0.8380,\n",
      "        0.7822, 0.8865, 0.8828, 0.8343, 0.9419, 0.8104, 0.7840, 0.8610, 0.9313,\n",
      "        0.9008, 0.9077, 0.8803, 0.8662, 0.7853, 0.8804, 0.8905, 0.8543, 0.8649,\n",
      "        0.8067, 0.8755, 0.8350, 0.8745, 0.2688, 0.7958, 0.1919, 0.8165, 0.0455,\n",
      "        0.7883, 0.7761, 0.9389, 0.8403, 0.7734, 0.6590, 0.8136, 0.7718, 0.8476,\n",
      "        0.8990, 0.7781])\n",
      "tensor([0.5525, 0.7975, 0.9322, 0.9263, 0.6515, 0.2419, 0.9047, 0.8080, 0.8728,\n",
      "        0.3370, 0.8886, 0.7917, 0.9372, 0.9104, 0.6111, 0.7828, 0.9586, 0.9000,\n",
      "        0.7273, 0.5193, 0.8634, 0.9392, 0.9171, 0.8665, 0.8214, 0.6840, 0.7784,\n",
      "        0.9210, 0.8553, 0.8879, 0.9206, 0.8060, 0.9244, 0.8679, 0.8753, 0.7500,\n",
      "        0.8537, 0.8406, 0.8949, 0.8160, 0.8567, 0.8374, 0.7576, 0.8538, 0.8424,\n",
      "        0.8472, 0.7765, 0.6799, 0.8257, 0.7454, 0.9167, 0.8104, 0.9336, 0.8448,\n",
      "        0.9035, 0.8459, 0.0311, 0.2847, 0.8876, 0.8680, 0.8989, 0.5443, 0.6488,\n",
      "        0.7305, 0.7921, 0.8059, 0.9186, 0.8400, 0.8009])\n",
      "tensor([0.9593, 0.7707, 0.8281, 0.7514, 0.8359, 0.9326, 0.7959, 0.7067, 0.7361,\n",
      "        0.7811, 0.8812, 0.8290, 0.9362, 0.6716, 0.8692, 0.9039, 0.9059, 0.8915,\n",
      "        0.8082, 0.8224, 0.7470, 0.7935, 0.5954, 0.9626, 0.7012, 0.9172, 0.9067,\n",
      "        0.8506, 0.8954, 0.8901, 0.7345, 0.7282, 0.9099, 0.7912, 0.9022, 0.8897,\n",
      "        0.8517, 0.8773, 0.9085, 0.8724, 0.8532, 0.7589, 0.8609, 0.8770, 0.8229,\n",
      "        0.9222, 0.7818, 0.6327, 0.8431, 0.8269, 0.9197, 0.9075, 0.8459, 0.8524,\n",
      "        0.8453, 0.8731, 0.8735, 0.9230, 0.6479, 0.9308, 0.6907])\n",
      "tensor([0.7096, 0.8025, 0.8675, 0.9581, 0.8019, 0.5757, 0.9264, 0.4519, 0.8432,\n",
      "        0.9076, 0.3261, 0.8801, 0.8371, 0.8596, 0.9085, 0.9278, 0.8755, 0.9292,\n",
      "        0.7939, 0.8222, 0.8926, 0.8806, 0.8830, 0.8356, 0.2179, 0.8977, 0.8959,\n",
      "        0.7875, 0.7233, 0.8552, 0.8141, 0.7809, 0.9211, 0.9137, 0.9045, 0.8845,\n",
      "        0.8623, 0.7373, 0.8769, 0.8088, 0.9060, 0.8620, 0.8625, 0.8830, 0.8999,\n",
      "        0.7912, 0.8332, 0.8574, 0.3840, 0.8078, 0.0934, 0.8290, 0.8188, 0.8250,\n",
      "        0.0410, 0.8556, 0.9151, 0.8709, 0.9131, 0.9551, 0.9099, 0.7215, 0.8251,\n",
      "        0.6864, 0.8662])\n",
      "tensor([0.9439, 0.9235, 0.4988, 0.7647, 0.8008, 0.7721, 0.7575, 0.9270, 0.7632,\n",
      "        0.7509, 0.8926, 0.8881, 0.0249, 0.2840, 0.8855, 0.4544, 0.6364, 0.8792,\n",
      "        0.4196, 0.8179, 0.9046, 0.8660, 0.8187, 0.8599, 0.8541, 0.8898, 0.8184,\n",
      "        0.5642, 0.8147, 0.8334, 0.8277, 0.8881, 0.7781, 0.8302, 0.3259, 0.7058,\n",
      "        0.8681, 0.8674, 0.8893, 0.8306, 0.8849, 0.9017, 0.9104, 0.7598, 0.3609,\n",
      "        0.7330, 0.7784, 0.8860, 0.9103, 0.8141, 0.7271, 0.8097, 0.0821, 0.8315,\n",
      "        0.2824, 0.8757, 0.9317, 0.8643, 0.8811, 0.8734, 0.7332, 0.8128, 0.6705])\n",
      "tensor([0.9409, 0.8906, 0.4097, 0.8000, 0.8369, 0.8039, 0.8517, 0.9323, 0.8344,\n",
      "        0.8152, 0.6939, 0.7781, 0.8480, 0.0843, 0.5304, 0.8541, 0.6136, 0.8468,\n",
      "        0.7495, 0.8412, 0.5844, 0.7635, 0.8977, 0.7566, 0.8438, 0.7497, 0.8421,\n",
      "        0.6383, 0.8862, 0.6137, 0.7600, 0.9028, 0.9414, 0.8238, 0.9005, 0.8448,\n",
      "        0.8404, 0.6677, 0.7401, 0.8765, 0.8318, 0.8862, 0.8307, 0.8701, 0.8869,\n",
      "        0.8924, 0.7491, 0.6054, 0.6702, 0.7294, 0.8840, 0.8457, 0.6633, 0.5706,\n",
      "        0.3962, 0.1924, 0.6968, 0.6623, 0.8423, 0.8957, 0.8238, 0.8346, 0.9170,\n",
      "        0.7359, 0.8592, 0.9003])\n",
      "tensor([0.9536, 0.7305, 0.5551, 0.6415, 0.7500, 0.8528, 0.8533, 0.9092, 0.9613,\n",
      "        0.8905, 0.4077, 0.9039, 0.8767, 0.8384, 0.7990, 0.8263, 0.8559, 0.8448,\n",
      "        0.8345, 0.7971, 0.8908, 0.8622, 0.8862, 0.7951, 0.0226, 0.9178, 0.9129,\n",
      "        0.8588, 0.8357, 0.7132, 0.9361, 0.8864, 0.8966, 0.8771, 0.7481, 0.8327,\n",
      "        0.8832, 0.8105, 0.8945, 0.8293, 0.8056, 0.8717, 0.8497, 0.8513, 0.8993,\n",
      "        0.7037, 0.7382, 0.8801, 0.8829, 0.7567, 0.7596, 0.8863, 0.7373, 0.8895,\n",
      "        0.7917, 0.9313, 0.8479, 0.8288, 0.7118, 0.8254, 0.8896, 0.7921, 0.8717,\n",
      "        0.9139])\n",
      "tensor([0.7783, 0.4576, 0.7796, 0.7847, 0.7189, 0.9076, 0.9391, 0.1533, 0.9579,\n",
      "        0.9612, 0.8553, 0.7463, 0.8656, 0.9013, 0.9121, 0.9164, 0.8767, 0.8941,\n",
      "        0.9190, 0.8418, 0.9236, 0.8977, 0.8105, 0.8800, 0.8967, 0.8437, 0.9080,\n",
      "        0.9632, 0.8633, 0.9114, 0.8889, 0.8008, 0.8974, 0.7895, 0.9139, 0.8366,\n",
      "        0.9311, 0.8688, 0.1527, 0.7999, 0.8150, 0.8747, 0.8525, 0.4360, 0.8326,\n",
      "        0.8977, 0.7400, 0.8597, 0.9187, 0.8099, 0.8612, 0.0066, 0.3227, 0.3680,\n",
      "        0.8609, 0.8176, 0.9220, 0.8822, 0.7933, 0.8787, 0.6388, 0.9125, 0.8906,\n",
      "        0.8707, 0.8139, 0.8946, 0.8263, 0.8985])\n",
      "Validation IOU at iteration 0: 0.0\n",
      "Training IOU at iteration 2: 0.01764600979089737\n",
      "tensor([0.7663, 0.7372, 0.7329, 0.8892, 0.7229, 0.8300, 0.9181, 0.1601, 0.9654,\n",
      "        0.8214, 0.9356, 0.8120, 0.9003, 0.9105, 0.8226, 0.8491, 0.9126, 0.8748,\n",
      "        0.7729, 0.8703, 0.6771, 0.9198, 0.8493, 0.8599, 0.8989, 0.9197, 0.8616,\n",
      "        0.8448, 0.9264, 0.9103, 0.8714, 0.8132, 0.8377, 0.9132, 0.8387, 0.8898,\n",
      "        0.9227, 0.8711, 0.8536, 0.4523, 0.7905, 0.8565, 0.7972, 0.8817, 0.3696,\n",
      "        0.8933, 0.8540, 0.8315, 0.7910, 0.9294, 0.8409, 0.8587, 0.0228, 0.3004,\n",
      "        0.8866, 0.8680, 0.7671, 0.9123, 0.9065, 0.8728, 0.8770, 0.8497, 0.7425,\n",
      "        0.8543, 0.8892, 0.9467, 0.7381, 0.8646, 0.8957, 0.8185, 0.8790])\n",
      "tensor([0.9457, 0.3064, 0.8807, 0.9031, 0.9053, 0.9182, 0.7298, 0.4380, 0.6200,\n",
      "        0.8467, 0.9244, 0.8426, 0.9238, 0.5893, 0.2238, 0.8382, 0.9183, 0.8177,\n",
      "        0.8544, 0.7813, 0.1464, 0.7039, 0.8633, 0.8795, 0.8365, 0.9106, 0.7365,\n",
      "        0.9087, 0.8179, 0.8635, 0.7430, 0.7394, 0.9118, 0.8111, 0.7609, 0.7710,\n",
      "        0.8743, 0.8495, 0.9090, 0.7720, 0.9211, 0.9135, 0.9266, 0.9061, 0.9023,\n",
      "        0.8615, 0.9034, 0.8125, 0.9213, 0.9403, 0.9020, 0.9063, 0.8281, 0.8286,\n",
      "        0.7120, 0.8659, 0.9159, 0.5674, 0.7122, 0.8833, 0.8610, 0.8295, 0.0279,\n",
      "        0.8156, 0.1915, 0.9205, 0.4762, 0.3170, 0.8345, 0.8874, 0.9386, 0.7303,\n",
      "        0.9357, 0.8472])\n",
      "tensor([0.9637, 0.6606, 0.6008, 0.5993, 0.9236, 0.7905, 0.7503, 0.9312, 0.8922,\n",
      "        0.4115, 0.8706, 0.8204, 0.9369, 0.8931, 0.6824, 0.8406, 0.8791, 0.8556,\n",
      "        0.9342, 0.9076, 0.9070, 0.9230, 0.0695, 0.9506, 0.9095, 0.8754, 0.8134,\n",
      "        0.8009, 0.8225, 0.9116, 0.8814, 0.8881, 0.9007, 0.8061, 0.8705, 0.8624,\n",
      "        0.8398, 0.8551, 0.8849, 0.7574, 0.8571, 0.8159, 0.8190, 0.8534, 0.8667,\n",
      "        0.7647, 0.8015, 0.8133, 0.7898, 0.7417, 0.8452, 0.8093, 0.7148, 0.8692,\n",
      "        0.8263, 0.7788, 0.8841, 0.8248, 0.8179, 0.8682, 0.7493, 0.8121, 0.8207,\n",
      "        0.8843])\n",
      "tensor([0.9477, 0.9115, 0.7677, 0.7395, 0.9407, 0.8138, 0.7343, 0.8637, 0.8350,\n",
      "        0.8084, 0.9246, 0.9123, 0.7219, 0.7995, 0.1638, 0.9304, 0.8848, 0.5417,\n",
      "        0.8632, 0.8187, 0.7145, 0.8271, 0.8914, 0.9295, 0.9157, 0.8831, 0.8380,\n",
      "        0.7686, 0.8381, 0.9089, 0.8683, 0.8792, 0.8519, 0.7820, 0.9281, 0.7070,\n",
      "        0.9253, 0.7186, 0.8602, 0.9070, 0.8907, 0.6976, 0.8265, 0.8319, 0.8415,\n",
      "        0.8768, 0.8308, 0.8615, 0.8983, 0.9160, 0.8509, 0.3286, 0.8879, 0.7761,\n",
      "        0.8306, 0.8681, 0.9391, 0.8474, 0.8498, 0.6971, 0.7817, 0.9268, 0.8292,\n",
      "        0.6714, 0.4639])\n"
     ]
    }
   ],
   "source": [
    "# - Finetuning by Using Points, Boxes, Mask Logits\n",
    "train_loss_file = open(os.path.join(log_dir, \"sam_train_loss.txt\"), \"w\")\n",
    "val_loss_file = open(os.path.join(log_dir, \"sam_val_loss.txt\"), \"w\")\n",
    "for itr in range(itrs):\n",
    "    with torch.amp.autocast(device_type='cpu'):\n",
    "        image, mask, input_point, input_boxes, input_label = read_batch(data)\n",
    "        if mask.shape[0] == 0: continue\n",
    "        predictor.set_image(image)\n",
    "\n",
    "        reshaped_masks = resize_masks_opencv(mask)\n",
    "\n",
    "\n",
    "        # - prompt encoding\n",
    "        mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(input_point, input_label, box=input_boxes, mask_logits=reshaped_masks, normalize_coords=True)\n",
    "        sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(points=(unnorm_coords, labels), boxes=unnorm_box, masks=mask_input)\n",
    "\n",
    "\n",
    "\n",
    "        # - mask decoder\n",
    "        batched_mode = unnorm_coords.shape[0] > 1\n",
    "        high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n",
    "        low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n",
    "            image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),\n",
    "            image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=True,\n",
    "            repeat_image=batched_mode,\n",
    "            high_res_features=[feat for feat in high_res_features],\n",
    "        )\n",
    "\n",
    "\n",
    "        prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])\n",
    "\n",
    "        # - segmentation loss calculation on CPU\n",
    "        gt_mask = torch.tensor(mask.astype(np.float32))[:, :, :, 0]\n",
    "        prd_mask = torch.sigmoid(prd_masks[:, 0])\n",
    "        seg_loss = (-gt_mask * torch.log(prd_mask + 0.00001) - (1 - gt_mask) * torch.log((1 - prd_mask) + 0.00001)).mean()\n",
    "\n",
    "        # - score loss calculation (intersection over union) IOU on CPU\n",
    "        inter = (gt_mask * (prd_mask > 0.5)).sum(1).sum(1)\n",
    "        iou = inter / (gt_mask.sum(1).sum(1) + (prd_mask > 0.5).sum(1).sum(1) - inter)\n",
    "        score_loss = torch.abs(prd_scores[:, 0] - iou).mean()\n",
    "\n",
    "        # Move the total loss to GPU for backpropagation\n",
    "        loss = (seg_loss + score_loss * 0.05)\n",
    "        predictor.model.zero_grad()  # empty gradient\n",
    "        scaler.scale(loss).backward()  # Backpropagate\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()  # Mix precision\n",
    "\n",
    "        if (itr + 1) % 1000 == 0:\n",
    "            torch.save(predictor.model.state_dict(), f\"./checkpoints/all/large_model_slice_{itr + 1}.torch\")\n",
    "            print(\"save model\")\n",
    "\n",
    "        if itr == 0:\n",
    "            mean_iou = 0\n",
    "        mean_iou = mean_iou * 0.99 + 0.01 * np.mean(iou.cpu().detach().numpy())\n",
    "        print(f\"Training IOU at iteration {itr + 1}: {mean_iou}\")\n",
    "        # Save training loss for this epoch\n",
    "        train_loss_file.write(f\"{itr + 1},{mean_iou}\\n\")\n",
    "        train_loss_file.flush()\n",
    "    \n",
    "    # - Evaluation Step\n",
    "    # Evaluation step on validation data after each iteration\n",
    "\n",
    "    total_iou = 0\n",
    "    for i in range(val_num):\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            img, mask, input_points, input_boxes, input_labels = read_batch(valid_data)\n",
    "\n",
    "            predictor.set_image(img)  # Set image in the predictor (Image Encoder)\n",
    "\n",
    "            # Prompt Encoder + Mask Decoder\n",
    "            masks, scores, logits = predictor.predict(\n",
    "                point_coords=input_points,\n",
    "                point_labels=input_labels,\n",
    "                box=input_boxes,\n",
    "                multimask_output=False\n",
    "            )\n",
    "\n",
    "            prd_mask = torch.sigmoid(torch.tensor(masks[:, 0], dtype=torch.float32))\n",
    "            gt_mask = torch.tensor(mask.astype(np.float32))[:, :, :, 0]\n",
    "\n",
    "            # Calculate IOU for validation data\n",
    "            inter = (gt_mask * (prd_mask > 0.5)).sum(1).sum(1)\n",
    "            iou_val = inter / (gt_mask.sum(1).sum(1) + (prd_mask > 0.5).sum(1).sum(1) - inter)\n",
    "\n",
    "            total_iou += iou_val.mean().cpu().numpy()\n",
    "            print(total_iou)\n",
    "    \n",
    "    \n",
    "    avg_iou = total_iou / val_num\n",
    "    print(f\"Validation IOU at iteration {itr}: {avg_iou}\")\n",
    "\n",
    "    # Save validation loss for this epoch\n",
    "    val_loss_file.write(f\"{itr + 1},{avg_iou}\\n\")\n",
    "    val_loss_file.flush()\n",
    "# Close the log files\n",
    "train_loss_file.close()\n",
    "val_loss_file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuxuan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
