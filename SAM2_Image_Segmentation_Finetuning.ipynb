{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM2 Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.ops import masks_to_boxes\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAM2 Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Global Variables\n",
    "\n",
    "data_dir= Path(\"./snemi/\" )\n",
    "raw_image_dir = data_dir / 'image_pngs'\n",
    "seg_image_dir = data_dir / 'seg_pngs'\n",
    "\n",
    "sam2_checkpoint = \"./sam2_hiera_large.pt\"\n",
    "model_cfg = \"./sam2_hiera_l.yaml\"\n",
    "itrs = 10000\n",
    "\n",
    "checkpoint_dir = './checkpoints/all'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs('./checkpoints/all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Prepare dataset\n",
    "data = []\n",
    "for ff, name in enumerate(os.listdir(raw_image_dir)):\n",
    "    data.append({'image': raw_image_dir / f'image{ff:04d}.png', 'annotation': seg_image_dir / f'seg{ff:04d}.png'})\n",
    "# - split train dataset and validation dataset\n",
    "valid_data = data[80:]\n",
    "data = data[:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: (1024, 1024, 3)\n",
      "annotation shape: (1024, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "# - Read Data\n",
    "\n",
    "def read_batch(data):\n",
    "     #  select image\n",
    "     ent  = data[np.random.randint(len(data))] # choose random entry\n",
    "     Img = cv2.imread(str(ent[\"image\"])) # read image\n",
    "     ann_map_grayscale = np.array(Image.open(ent['annotation']))\n",
    "     ann_map = np.stack((ann_map_grayscale, ) * 3, axis = -1)\n",
    "     print(f'image shape: {np.array(Img).shape}')\n",
    "     print(f'annotation shape: {np.array(ann_map).shape}')\n",
    "     \n",
    "     # resize image\n",
    "     r = np.min([1024 / Img.shape[1], 1024 / Img.shape[0]]) # scalling factor\n",
    "     Img = cv2.resize(Img, (int(Img.shape[1] * r), int(Img.shape[0] * r)))\n",
    "     ann_map = cv2.resize(ann_map, (int(ann_map.shape[1] * r), int(ann_map.shape[0] * r)),interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "     # - get bounding box\n",
    "     inds = np.unique(ann_map_grayscale)[1:] # load all indices\n",
    "\n",
    "     masks = [] \n",
    "     for ind in inds:\n",
    "        masks.append(ann_map_grayscale == ind)\n",
    "     masks = np.array(masks)\n",
    "     masks_tensor = torch.from_numpy(masks)\n",
    "\n",
    "     boxes = masks_to_boxes(masks_tensor)\n",
    "     input_boxes = boxes.numpy()\n",
    "\n",
    "\n",
    "\n",
    "     # Get binary masks and points\n",
    "     mat_map = ann_map\n",
    "     inds = np.unique(mat_map)[1:] # load all indices\n",
    "     points= []\n",
    "     masks = [] \n",
    "     for ind in inds:\n",
    "          mask=(mat_map == ind).astype(np.uint8) # make binary mask\n",
    "          masks.append(mask)\n",
    "          coords = np.argwhere(mask > 0) # get all coordinates in mask\n",
    "          yx = np.array(coords[np.random.randint(len(coords))]) # choose random point/coordinate\n",
    "          points.append([[yx[1], yx[0]]])\n",
    "     return Img,np.array(masks),np.array(points), input_boxes, np.ones([len(masks),1])\n",
    "img, mask_arr,  point_arr, input_boxes, one_arr= read_batch(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Finetuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Build Model\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device='cpu')\n",
    "predictor = SAM2ImagePredictor(sam2_model)\n",
    "# - Set Training Parameters\n",
    "predictor.model.sam_mask_decoder.train(True)\n",
    "predictor.model.sam_prompt_encoder.train(True)\n",
    "predictor.model.image_encoder.train(True)\n",
    "# - Set Optimizer and Scaler\n",
    "optimizer=torch.optim.AdamW(params=predictor.model.parameters(),lr=1e-5,weight_decay=4e-5)\n",
    "scaler = torch.amp.GradScaler()\n",
    "# - Force cuda operations to be synchronized\n",
    "!export CUDA_LAUNCH_BLOCKING=1\n",
    "# - Define Devices\n",
    "device0 = torch.device('cuda:0')\n",
    "device1 = torch.device('cuda:1')\n",
    "device_cpu = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: (1024, 1024, 3)\n",
      "annotation shape: (1024, 1024, 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m itr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(itrs):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m         image, mask, input_point, input_boxes, input_label \u001b[38;5;241m=\u001b[39m \u001b[43mread_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         predictor\u001b[38;5;241m.\u001b[39mset_image(image)\n",
      "Cell \u001b[0;32mIn[17], line 37\u001b[0m, in \u001b[0;36mread_batch\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     35\u001b[0m masks \u001b[38;5;241m=\u001b[39m [] \n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m inds:\n\u001b[0;32m---> 37\u001b[0m      mask\u001b[38;5;241m=\u001b[39m\u001b[43m(\u001b[49m\u001b[43mmat_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# make binary mask\u001b[39;00m\n\u001b[1;32m     38\u001b[0m      masks\u001b[38;5;241m.\u001b[39mappend(mask)\n\u001b[1;32m     39\u001b[0m      coords \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margwhere(mask \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# get all coordinates in mask\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # - Finetuning by Using Input Points\n",
    "# for itr in range(itrs):\n",
    "#     with torch.amp.autocast(device_type='cpu'):\n",
    "#         image, mask, input_point, input_boxes, input_label = read_batch(data)\n",
    "#         if mask.shape[0] == 0: continue\n",
    "#         predictor.set_image(image)\n",
    "\n",
    "#         # - prompt encoding\n",
    "#         mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(input_point, input_label, box=None, mask_logits=None, normalize_coords=True)\n",
    "#         sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(points=(unnorm_coords, labels), boxes=None, masks=None)\n",
    "\n",
    "\n",
    "#         # - mask decoder\n",
    "#         batched_mode = unnorm_coords.shape[0] > 1\n",
    "#         high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n",
    "#         low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n",
    "#             image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),\n",
    "#             image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n",
    "#             sparse_prompt_embeddings=sparse_embeddings,\n",
    "#             dense_prompt_embeddings=dense_embeddings,\n",
    "#             multimask_output=True,\n",
    "#             repeat_image=batched_mode,\n",
    "#             high_res_features=[feat for feat in high_res_features],\n",
    "#         )\n",
    "\n",
    "\n",
    "#         prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])\n",
    "\n",
    "#         # - segmentation loss calculation on CPU\n",
    "#         gt_mask = torch.tensor(mask.astype(np.float32))[:, :, :, 0]\n",
    "#         prd_mask = torch.sigmoid(prd_masks[:, 0])\n",
    "#         seg_loss = (-gt_mask * torch.log(prd_mask + 0.00001) - (1 - gt_mask) * torch.log((1 - prd_mask) + 0.00001)).mean()\n",
    "\n",
    "#         # - score loss calculation (intersection over union) IOU on CPU\n",
    "#         inter = (gt_mask * (prd_mask > 0.5)).sum(1).sum(1)\n",
    "#         iou = inter / (gt_mask.sum(1).sum(1) + (prd_mask > 0.5).sum(1).sum(1) - inter)\n",
    "#         score_loss = torch.abs(prd_scores[:, 0] - iou).mean()\n",
    "\n",
    "#         # Move the total loss to GPU for backpropagation\n",
    "#         loss = (seg_loss + score_loss * 0.05)\n",
    "#         predictor.model.zero_grad()  # empty gradient\n",
    "#         scaler.scale(loss).backward()  # Backpropagate\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()  # Mix precision\n",
    "\n",
    "#         if itr % 1000 == 0:\n",
    "#             torch.save(predictor.model.state_dict(), f\"./checkpoints/large_model_point_{itr}.torch\")\n",
    "#             print(\"save model\")\n",
    "\n",
    "#         if itr == 0:\n",
    "#             mean_iou = 0\n",
    "#         mean_iou = mean_iou * 0.99 + 0.01 * np.mean(iou.cpu().detach().numpy())\n",
    "#         print(\"step)\", itr, \"Accuracy(IOU)=\", mean_iou)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # - Save the newest one checkpoint in Input Point Finetuning\n",
    "# torch.save(predictor.model.state_dict(), f\"./checkpoints/large_model_point_{itr}.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: (1024, 1024, 3)\n",
      "annotation shape: (1024, 1024, 3)\n",
      "save model\n",
      "step) 0 Accuracy(IOU)= 0.005523698925971985\n",
      "image shape: (1024, 1024, 3)\n",
      "annotation shape: (1024, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "# - Finetuning by Using Points, Boxes, Mask Logits\n",
    "for itr in range(itrs):\n",
    "    with torch.amp.autocast(device_type='cpu'):\n",
    "        image, mask, input_point, input_boxes, input_label = read_batch(data)\n",
    "        if mask.shape[0] == 0: continue\n",
    "        predictor.set_image(image)\n",
    "\n",
    "        # - prompt encoding\n",
    "        mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(input_point, input_label, box=input_boxes, mask_logits=mask, normalize_coords=True)\n",
    "        sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(points=(unnorm_coords, labels), boxes=None, masks=None)\n",
    "\n",
    "\n",
    "        # - mask decoder\n",
    "        batched_mode = unnorm_coords.shape[0] > 1\n",
    "        high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n",
    "        low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n",
    "            image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),\n",
    "            image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=True,\n",
    "            repeat_image=batched_mode,\n",
    "            high_res_features=[feat for feat in high_res_features],\n",
    "        )\n",
    "\n",
    "\n",
    "        prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])\n",
    "\n",
    "        # - segmentation loss calculation on CPU\n",
    "        gt_mask = torch.tensor(mask.astype(np.float32))[:, :, :, 0]\n",
    "        prd_mask = torch.sigmoid(prd_masks[:, 0])\n",
    "        seg_loss = (-gt_mask * torch.log(prd_mask + 0.00001) - (1 - gt_mask) * torch.log((1 - prd_mask) + 0.00001)).mean()\n",
    "\n",
    "        # - score loss calculation (intersection over union) IOU on CPU\n",
    "        inter = (gt_mask * (prd_mask > 0.5)).sum(1).sum(1)\n",
    "        iou = inter / (gt_mask.sum(1).sum(1) + (prd_mask > 0.5).sum(1).sum(1) - inter)\n",
    "        score_loss = torch.abs(prd_scores[:, 0] - iou).mean()\n",
    "\n",
    "        # Move the total loss to GPU for backpropagation\n",
    "        loss = (seg_loss + score_loss * 0.05)\n",
    "        predictor.model.zero_grad()  # empty gradient\n",
    "        scaler.scale(loss).backward()  # Backpropagate\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()  # Mix precision\n",
    "\n",
    "        if itr % 1000 == 0:\n",
    "            torch.save(predictor.model.state_dict(), f\"./checkpoints/all/large_model_all_{itr}.torch\")\n",
    "            print(\"save model\")\n",
    "\n",
    "        if itr == 0:\n",
    "            mean_iou = 0\n",
    "        mean_iou = mean_iou * 0.99 + 0.01 * np.mean(iou.cpu().detach().numpy())\n",
    "        print(\"step)\", itr, \"Accuracy(IOU)=\", mean_iou)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Save the newest checkpoint\n",
    "torch.save(predictor.model.state_dict(), f\"./checkpoints/all/large_model_all_{itr}.torch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuxuan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
