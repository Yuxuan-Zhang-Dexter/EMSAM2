{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM2 Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.ops import masks_to_boxes\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAM2 Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Global Variables\n",
    "\n",
    "data_dir= Path(\"./snemi/\" )\n",
    "raw_image_dir = data_dir / 'image_pngs'\n",
    "seg_image_dir = data_dir / 'seg_pngs'\n",
    "\n",
    "sam2_checkpoint = \"./sam2_hiera_large.pt\"\n",
    "model_cfg = \"./sam2_hiera_l.yaml\"\n",
    "itrs = 10000\n",
    "\n",
    "checkpoint_dir = './checkpoints/all'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs('./checkpoints/all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Prepare dataset\n",
    "data = []\n",
    "for ff, name in enumerate(os.listdir(raw_image_dir)):\n",
    "    data.append({'image': raw_image_dir / f'image{ff:04d}.png', 'annotation': seg_image_dir / f'seg{ff:04d}.png'})\n",
    "# - split train dataset and validation dataset\n",
    "valid_data = data[80:]\n",
    "data = data[:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: (1024, 1024, 3)\n",
      "annotation shape: (1024, 1024, 3)\n",
      "mask_arr shape: (221, 1024, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "# - Read Data\n",
    "\n",
    "def read_batch(data):\n",
    "     #  select image\n",
    "     ent  = data[np.random.randint(len(data))] # choose random entry\n",
    "     Img = cv2.imread(str(ent[\"image\"])) # read image\n",
    "     ann_map_grayscale = np.array(Image.open(ent['annotation']))\n",
    "     ann_map = np.stack((ann_map_grayscale, ) * 3, axis = -1)\n",
    "     print(f'image shape: {np.array(Img).shape}')\n",
    "     print(f'annotation shape: {np.array(ann_map).shape}')\n",
    "     \n",
    "     # resize image\n",
    "     r = np.min([1024 / Img.shape[1], 1024 / Img.shape[0]]) # scalling factor\n",
    "     Img = cv2.resize(Img, (int(Img.shape[1] * r), int(Img.shape[0] * r)))\n",
    "     ann_map = cv2.resize(ann_map, (int(ann_map.shape[1] * r), int(ann_map.shape[0] * r)),interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "     # - get bounding box\n",
    "     inds = np.unique(ann_map_grayscale)[1:] # load all indices\n",
    "\n",
    "     masks = [] \n",
    "     for ind in inds:\n",
    "        masks.append(ann_map_grayscale == ind)\n",
    "     masks = np.array(masks)\n",
    "     masks_tensor = torch.from_numpy(masks)\n",
    "\n",
    "     boxes = masks_to_boxes(masks_tensor)\n",
    "     input_boxes = boxes.numpy()\n",
    "\n",
    "\n",
    "\n",
    "     # Get binary masks and points\n",
    "     mat_map = ann_map\n",
    "     inds = np.unique(mat_map)[1:] # load all indices\n",
    "     points= []\n",
    "     masks = [] \n",
    "     for ind in inds:\n",
    "          mask=(mat_map == ind).astype(np.uint8) # make binary mask\n",
    "          masks.append(mask)\n",
    "          coords = np.argwhere(mask > 0) # get all coordinates in mask\n",
    "          yx = np.array(coords[np.random.randint(len(coords))]) # choose random point/coordinate\n",
    "          points.append([[yx[1], yx[0]]])\n",
    "     return Img,np.array(masks),np.array(points), input_boxes, np.ones([len(masks),1])\n",
    "img, mask_arr,  point_arr, input_boxes, one_arr= read_batch(data)\n",
    "print(f'mask_arr shape: {mask_arr.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Finetuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Build Model\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device='cpu')\n",
    "predictor = SAM2ImagePredictor(sam2_model)\n",
    "# - Set Training Parameters\n",
    "predictor.model.sam_mask_decoder.train(True)\n",
    "predictor.model.sam_prompt_encoder.train(True)\n",
    "predictor.model.image_encoder.train(True)\n",
    "# - Set Optimizer and Scaler\n",
    "optimizer=torch.optim.AdamW(params=predictor.model.parameters(),lr=1e-5,weight_decay=4e-5)\n",
    "scaler = torch.amp.GradScaler()\n",
    "# - Force cuda operations to be synchronized\n",
    "!export CUDA_LAUNCH_BLOCKING=1\n",
    "# - Define Devices\n",
    "device0 = torch.device('cuda:0')\n",
    "device1 = torch.device('cuda:1')\n",
    "device_cpu = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: (1024, 1024, 3)\n",
      "annotation shape: (1024, 1024, 3)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (256) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m batched_mode \u001b[38;5;241m=\u001b[39m unnorm_coords\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     17\u001b[0m high_res_features \u001b[38;5;241m=\u001b[39m [feat_level[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m feat_level \u001b[38;5;129;01min\u001b[39;00m predictor\u001b[38;5;241m.\u001b[39m_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhigh_res_feats\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m---> 18\u001b[0m low_res_masks, prd_scores, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msam_mask_decoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_features\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage_embed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_pe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msam_prompt_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dense_pe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhigh_res_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhigh_res_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m prd_masks \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39m_transforms\u001b[38;5;241m.\u001b[39mpostprocess_masks(low_res_masks, predictor\u001b[38;5;241m.\u001b[39m_orig_hw[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# - segmentation loss calculation on CPU\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/yuxuan_exp/yuxuan_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/yuxuan_exp/yuxuan_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/yuxuan_exp/SAM2_finetuning/segment-anything-2/sam2/modeling/sam/mask_decoder.py:136\u001b[0m, in \u001b[0;36mMaskDecoder.forward\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, multimask_output, repeat_image, high_res_features)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    112\u001b[0m     image_embeddings: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m     high_res_features: Optional[List[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    119\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    Predict masks given image and prompt embeddings.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m      torch.Tensor: batched SAM token for mask output\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     masks, iou_pred, mask_tokens_out, object_score_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_masks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_pe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_pe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepeat_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhigh_res_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhigh_res_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Select the correct mask or masks for output\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m multimask_output:\n",
      "File \u001b[0;32m~/Desktop/yuxuan_exp/SAM2_finetuning/segment-anything-2/sam2/modeling/sam/mask_decoder.py:205\u001b[0m, in \u001b[0;36mMaskDecoder.predict_masks\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, repeat_image, high_res_features)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m image_embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    204\u001b[0m     src \u001b[38;5;241m=\u001b[39m image_embeddings\n\u001b[0;32m--> 205\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    207\u001b[0m     image_pe\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    208\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_pe should have size 1 in batch dim (from `get_dense_pe()`)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m pos_src \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(image_pe, tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (256) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "# - Finetuning by Using Points, Boxes, Mask Logits\n",
    "for itr in range(itrs):\n",
    "    with torch.amp.autocast(device_type='cpu'):\n",
    "        image, mask, input_point, input_boxes, input_label = read_batch(data)\n",
    "        if mask.shape[0] == 0: continue\n",
    "        predictor.set_image(image)\n",
    "\n",
    "        # - prompt encoding\n",
    "        mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(input_point, input_label, box=input_boxes, mask_logits=mask, normalize_coords=True)\n",
    "        sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(points=(unnorm_coords, labels), boxes=unnorm_box, masks=reshape_mask_input)\n",
    "\n",
    "\n",
    "\n",
    "        # - mask decoder\n",
    "        batched_mode = unnorm_coords.shape[0] > 1\n",
    "        high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n",
    "        low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n",
    "            image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),\n",
    "            image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=True,\n",
    "            repeat_image=batched_mode,\n",
    "            high_res_features=[feat for feat in high_res_features],\n",
    "        )\n",
    "\n",
    "\n",
    "        prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])\n",
    "\n",
    "        # - segmentation loss calculation on CPU\n",
    "        gt_mask = torch.tensor(mask.astype(np.float32))[:, :, :, 0]\n",
    "        prd_mask = torch.sigmoid(prd_masks[:, 0])\n",
    "        seg_loss = (-gt_mask * torch.log(prd_mask + 0.00001) - (1 - gt_mask) * torch.log((1 - prd_mask) + 0.00001)).mean()\n",
    "\n",
    "        # - score loss calculation (intersection over union) IOU on CPU\n",
    "        inter = (gt_mask * (prd_mask > 0.5)).sum(1).sum(1)\n",
    "        iou = inter / (gt_mask.sum(1).sum(1) + (prd_mask > 0.5).sum(1).sum(1) - inter)\n",
    "        score_loss = torch.abs(prd_scores[:, 0] - iou).mean()\n",
    "\n",
    "        # Move the total loss to GPU for backpropagation\n",
    "        loss = (seg_loss + score_loss * 0.05)\n",
    "        predictor.model.zero_grad()  # empty gradient\n",
    "        scaler.scale(loss).backward()  # Backpropagate\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()  # Mix precision\n",
    "\n",
    "        if (itr + 1) % 1000 == 0:\n",
    "            torch.save(predictor.model.state_dict(), f\"./checkpoints/all/large_model_all_{itr}.torch\")\n",
    "            print(\"save model\")\n",
    "\n",
    "        if itr == 0:\n",
    "            mean_iou = 0\n",
    "        mean_iou = mean_iou * 0.99 + 0.01 * np.mean(iou.cpu().detach().numpy())\n",
    "        print(\"step)\", itr, \"Accuracy(IOU)=\", mean_iou)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuxuan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
