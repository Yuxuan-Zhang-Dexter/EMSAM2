{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Florence2 Object Detection Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.ops import masks_to_boxes\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import supervision as sv\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    get_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Global Variables\n",
    "data_dir= Path(\"./snemi/\" )\n",
    "raw_image_dir = data_dir / 'image_pngs'\n",
    "seg_image_dir = data_dir / 'seg_pngs'\n",
    "test_image_dir = data_dir / 'image_test_pngs'\n",
    "\n",
    "\n",
    "CHECKPOINT = \"microsoft/Florence-2-base-ft\"\n",
    "REVISION = 'refs/pr/6'\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "label_num = 170\n",
    "\n",
    "## - Define Dataloader\n",
    "BATCH_SIZE = 5\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "checkpoint_dir = './model_checkpoints/epoch_1000'\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_dir, trust_remote_code=True).to(DEVICE)\n",
    "processor = AutoProcessor.from_pretrained(checkpoint_dir, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Prepare dataset\n",
    "# - Finetuning Dataset Processing\n",
    "## - Prepare dataset\n",
    "data = []\n",
    "for ff, name in enumerate(os.listdir(raw_image_dir)):\n",
    "    data.append({'image': raw_image_dir / f'image{ff:04d}.png', 'annotation': seg_image_dir / f'seg{ff:04d}.png'})\n",
    "# - split train dataset and validation dataset\n",
    "valid_data = data[80:]\n",
    "data = data[:80]\n",
    "\n",
    "## - Convert Mask to Bounding Boxes\n",
    "def convert_mask2box(mask:np.ndarray):\n",
    "    inds = np.unique(mask)[1:] # load all indices\n",
    "\n",
    "    masks = [] \n",
    "    for ind in inds:\n",
    "        masks.append(mask == ind)\n",
    "\n",
    "    masks = np.array(masks)\n",
    "    masks_tensor = torch.from_numpy(masks)\n",
    "\n",
    "    boxes = masks_to_boxes(masks_tensor)\n",
    "    valid_input_boxes = boxes.numpy()\n",
    "    return valid_input_boxes\n",
    "\n",
    "## - normalize location\n",
    "def normalize_loc(prefix:str, instance_type:str, image_path:str, mask:np.ndarray, input_boxes:np.ndarray):\n",
    "    x_res = mask.shape[0]\n",
    "    y_res = mask.shape[1]\n",
    "    normal_boxes = [[box[0] / x_res * 1000, box[1]/ y_res * 1000, box[2] / x_res * 1000, box[3] / x_res * 1000] for box in input_boxes]\n",
    "    normal_boxes = np.rint(normal_boxes)\n",
    "    suffix = ''\n",
    "    count = 0\n",
    "    for i in range(len(normal_boxes)):\n",
    "        #- reach the max sequence length 1024\n",
    "        if count == label_num:\n",
    "            break\n",
    "        x1 = int(normal_boxes[i][0])\n",
    "        y1 = int(normal_boxes[i][1])\n",
    "        x2 = int(normal_boxes[i][2])\n",
    "        y2 = int(normal_boxes[i][3])\n",
    "        suffix += f\"{instance_type}<loc_{x1}><loc_{y1}><loc_{x2}><loc_{y2}>\"\n",
    "        count += 1\n",
    "        \n",
    "\n",
    "    \n",
    "    return {\"image\": image_path,\"prefix\": prefix, \"suffix\": suffix }\n",
    "\n",
    "## - Prepare all training dataset and validation dataset\n",
    "def prepare_dataset(data, instance_type, prefix):\n",
    "    dataset = []\n",
    "    for element in data:\n",
    "        image_path = element['image']\n",
    "        seg_path = element['annotation']\n",
    "        mask = np.array(Image.open(seg_path))\n",
    "        input_boxes = convert_mask2box(mask)\n",
    "        curated_data = normalize_loc(prefix, instance_type, image_path, mask, input_boxes)\n",
    "        dataset.append(curated_data)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = prepare_dataset(data, 'neuron', \"<OD>\")\n",
    "val_dataset = prepare_dataset(valid_data, 'neuron', \"<OD>\")\n",
    "\n",
    "# - Initialize Dataset and Dataloader\n",
    "\n",
    "## - Detection Dataset Class (Dataset Preparation)\n",
    "class DetectionDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        image = cv2.imread(str(data['image']))\n",
    "        prefix = data['prefix']\n",
    "        suffix = data['suffix']\n",
    "        return prefix, suffix, image\n",
    "\n",
    "## - Define Dataloader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    questions, answers, images = zip(*batch)\n",
    "    inputs = processor(text=list(questions), images=list(images), return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    return inputs, answers\n",
    "\n",
    "train_dataset = DetectionDataset(train_dataset)\n",
    "val_dataset = DetectionDataset(val_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - EM Validation Image Sample Inference\n",
    "image = cv2.imread(str(valid_data[0]['image']))\n",
    "image = Image.fromarray(image)\n",
    "task = \"<OD>\"\n",
    "text = \"<OD>\"\n",
    "\n",
    "inputs = processor(\n",
    "    text=text, \n",
    "    images=image, \n",
    "    return_tensors=\"pt\"\n",
    ").to(DEVICE)\n",
    "generated_ids = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    pixel_values=inputs[\"pixel_values\"],\n",
    "    max_new_tokens=1024,\n",
    "    num_beams=3)\n",
    "generated_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=False)[0]\n",
    "response = processor.post_process_generation(\n",
    "    generated_text, \n",
    "    task=task, \n",
    "    image_size=image.size)\n",
    "detections = sv.Detections.from_lmm(\n",
    "    sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n",
    "\n",
    "bounding_box_annotator = sv.BoxAnnotator(\n",
    "    color_lookup=sv.ColorLookup.INDEX)\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "image = bounding_box_annotator.annotate(image, detections)\n",
    "image = label_annotator.annotate(image, detections)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - EM Test Image Sample Inference\n",
    "image = cv2.imread(str(test_image_dir / 'image0002.png'))\n",
    "image = Image.fromarray(image)\n",
    "task = \"<OD>\"\n",
    "text = \"<OD>\"\n",
    "\n",
    "inputs = processor(\n",
    "    text=text, \n",
    "    images=image, \n",
    "    return_tensors=\"pt\"\n",
    ").to(DEVICE)\n",
    "generated_ids = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    pixel_values=inputs[\"pixel_values\"],\n",
    "    max_new_tokens=1024,\n",
    "    num_beams=3)\n",
    "generated_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=False)[0]\n",
    "response = processor.post_process_generation(\n",
    "    generated_text, \n",
    "    task=task, \n",
    "    image_size=image.size)\n",
    "detections = sv.Detections.from_lmm(\n",
    "    sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n",
    "\n",
    "bounding_box_annotator = sv.BoxAnnotator(\n",
    "    color_lookup=sv.ColorLookup.INDEX)\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "image = bounding_box_annotator.annotate(image, detections)\n",
    "image = label_annotator.annotate(image, detections)\n",
    "plt.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuxuan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
